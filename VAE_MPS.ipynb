{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "VAE_MPS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lamd91/VAE_DA/blob/master/VAE_MPS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_8YcIzsMhdd"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "import random\n",
        "from IPython import display\n",
        "import os\n",
        "import shutil\n",
        "from PIL import Image as im"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlmkMoV9r0Zu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c680540a-403b-49e8-ea48-b8e75ac6fb3c"
      },
      "source": [
        "# Mount Google Drive at \"/content/gdrive\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vbMSVpOr8oS"
      },
      "source": [
        "After having uploaded `mps_simulations.tar.gz` located in the \"data\" folder of the github repository, extract the folder content here in Colab. The extracted files will be located in a directory named `images`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbWdJwWQHnwc"
      },
      "source": [
        "!tar -xf \"/content/gdrive/My Drive/mps_simulations.tar.gz\" \n",
        "!rm -r images/.* # delete hidden files or directories"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "i-JwifzONA43",
        "outputId": "3d9c480d-1b9d-438e-9e79-39bf0f2f5663"
      },
      "source": [
        "# Show one example of MPS simulation that will be used for training\n",
        "i = random.randint(0, 1999)\n",
        "image = pickle.load(open(f'images/img-{i}.pickle', 'rb')).astype('uint8')\n",
        "plt.imshow(image)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7eff348f3710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nejBn076s2So"
      },
      "source": [
        "# Define global constants\n",
        "BATCH_SIZE = 32\n",
        "LATENT_DIM = 2\n",
        "EPOCHS = 1000\n",
        "IMAGE_HEIGHT = 50\n",
        "IMAGE_WIDTH = 500\n",
        "NUM_CHANNELS = 1\n",
        "TEST_SIZE = 0.2\n",
        "SOURCE_DIR = 'images'\n",
        "TRAINING_DIR = 'training_images'\n",
        "TESTING_DIR = 'testing_images' "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpcXzUL6XSwp",
        "outputId": "40508ccd-6537-46b6-88b3-5113067a8f72"
      },
      "source": [
        "random.seed(42)\n",
        "\n",
        "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
        "    \"\"\"Split images into training and testing directories\"\"\"\n",
        "\n",
        "    image_filenames = os.listdir(SOURCE)\n",
        "    num_examples = len(image_filenames)\n",
        "    if not os.path.exists(os.path.join(TRAINING, 'input_images')):\n",
        "        os.makedirs(os.path.join(TRAINING, 'input_images'))\n",
        "    if not os.path.exists(os.path.join(TESTING, 'input_images')):\n",
        "        os.makedirs(os.path.join(TESTING, 'input_images'))\n",
        "\n",
        "    count = 0\n",
        "    num_training_examples = int(num_examples * (1-SPLIT_SIZE))\n",
        "    for file in random.sample(image_filenames, num_examples):\n",
        "        src = os.path.join(SOURCE, file)\n",
        "        if os.path.getsize(src) != 0:\n",
        "            if count < num_training_examples:\n",
        "                array = pickle.load(open(src, 'rb'))\n",
        "                img = im.fromarray(array.astype('uint8')*255, 'L')\n",
        "                img.save(os.path.join(TRAINING, 'input_images', file.replace('.pickle', '')) + '.png')\n",
        "            else:\n",
        "                array = pickle.load(open(src, 'rb'))\n",
        "                img = im.fromarray(array.astype('uint8')*255, 'L')\n",
        "                img.save(os.path.join(TESTING, 'input_images', file.replace('.pickle', '')) + '.png')\n",
        "            count += 1\n",
        "\n",
        "    print(\"Number of training examples:\", int(len(os.listdir(os.path.join(TRAINING, 'input_images')))))\n",
        "    print(\"Number of validation examples:\", int(len(os.listdir(os.path.join(TESTING, 'input_images')))))\n",
        "\n",
        "split_data(SOURCE_DIR, TRAINING_DIR, TESTING_DIR, TEST_SIZE)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 1600\n",
            "Number of validation examples: 400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klRQAqMks5PD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d12cae2d-2275-4e9d-e7d9-ee726ceb122f"
      },
      "source": [
        "def create_data_generators():\n",
        "    \"\"\"Create generators of training data and validation data\"\"\" \n",
        "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        rescale = 1.0/255.0,\n",
        "        width_shift_range = 0.15,\n",
        "        horizontal_flip = True,\n",
        "        vertical_flip = True,\n",
        "        fill_mode='nearest'    \n",
        "    )\n",
        "\n",
        "    val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        rescale = 1.0/255.0,\n",
        "    )\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        directory = TRAINING_DIR,\n",
        "        batch_size = BATCH_SIZE,\n",
        "        target_size = (50, 500),\n",
        "        color_mode = 'grayscale', # images will be converted to have 1 channel\n",
        "        class_mode = 'input', # labels will be images identical to input images\n",
        "        save_to_dir = '.'\n",
        "    )\n",
        "\n",
        "    val_generator = val_datagen.flow_from_directory(\n",
        "        directory = TESTING_DIR,\n",
        "        batch_size = BATCH_SIZE,\n",
        "        target_size = (50, 500),\n",
        "        color_mode = 'grayscale',\n",
        "        class_mode = 'input'\n",
        "    )\n",
        "\n",
        "    return train_generator, val_generator\n",
        "\n",
        "train_generator, val_generator = create_data_generators()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1600 images belonging to 1 classes.\n",
            "Found 400 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZReHg3QTtfe9"
      },
      "source": [
        "class Sampling(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Generates a random sample and combines with the encoder output\n",
        "\n",
        "        Args:\n",
        "          inputs -- output tensor from the encoder\n",
        "\n",
        "        Returns:\n",
        "          `inputs` tensors combined with a random sample\n",
        "        \"\"\"\n",
        "\n",
        "        # unpack the output of the encoder\n",
        "        mu, sigma = inputs\n",
        "\n",
        "        # get the size and dimensions of the batch\n",
        "        batch = tf.shape(mu)[0]\n",
        "        dim = tf.shape(mu)[1]\n",
        "\n",
        "        # generate a random tensor\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "\n",
        "        # combine the inputs and noise\n",
        "        return mu + tf.exp(0.5 * sigma) * epsilon\n",
        "\n",
        "\n",
        "def encoder_layers(inputs, latent_dim):\n",
        "    \"\"\"Defines the encoder's layers.\n",
        "    Args:\n",
        "      inputs -- batch from the dataset\n",
        "      latent_dim -- dimensionality of the latent space\n",
        "\n",
        "    Returns:\n",
        "      mu -- learned mean\n",
        "      sigma -- learned standard deviation\n",
        "      batch_2.shape -- shape of the features before flattening\n",
        "    \"\"\"\n",
        "\n",
        "    # add the Conv2D layers followed by BatchNormalization\n",
        "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, \n",
        "                               padding=\"same\", activation='relu',\n",
        "                               name=\"encode_conv1\")(inputs)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, \n",
        "                               padding='same', activation='relu',\n",
        "                               name=\"encode_conv2\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=2, \n",
        "                               padding='same', activation='relu',\n",
        "                               name=\"encode_conv3\")(x)\n",
        "    # x = tf.keras.layers.BatchNormalization()(x)\n",
        "    # x = tf.keras.layers.Conv2D(filters=256, kernel_size=3, strides=2, \n",
        "    #                            padding='same', activation='relu',\n",
        "    #                            name=\"encode_conv4\")(x)    \n",
        "\n",
        "    # assign to a different variable so you can extract the shape later\n",
        "    batch_3 = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    # flatten the features and feed into the Dense network\n",
        "    x = tf.keras.layers.Flatten(name=\"encode_flatten\")(batch_3)\n",
        "\n",
        "    # we arbitrarily used ... units here but feel free to change\n",
        "    x = tf.keras.layers.Dense(1024, activation='relu', name=\"encode_dense\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    # add output Dense networks for mu and sigma, units equal to the declared latent_dim.\n",
        "    mu = tf.keras.layers.Dense(latent_dim, name='latent_mu')(x)\n",
        "    sigma = tf.keras.layers.Dense(latent_dim, name='latent_sigma')(x)\n",
        "\n",
        "    return mu, sigma, batch_3.shape\n",
        "\n",
        "def encoder_model(latent_dim, input_shape):\n",
        "    \"\"\"Defines the encoder model with the Sampling layer\n",
        "    Args:\n",
        "      latent_dim -- dimensionality of the latent space\n",
        "      input_shape -- shape of the dataset batch\n",
        "\n",
        "    Returns:\n",
        "      model -- the encoder model\n",
        "      conv_shape -- shape of the features before flattening\n",
        "    \"\"\"\n",
        "\n",
        "    # declare the inputs tensor with the given shape\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "    # get the output of the encoder_layers() function\n",
        "    mu, sigma, conv_shape = encoder_layers(inputs, latent_dim=LATENT_DIM)\n",
        "\n",
        "    # feed mu and sigma to the Sampling layer\n",
        "    z = Sampling()((mu, sigma))\n",
        "\n",
        "    # build the whole encoder model\n",
        "    model = tf.keras.Model(inputs, outputs=[mu, sigma, z])\n",
        "\n",
        "    return model, conv_shape\n",
        "\n",
        "\n",
        "# Define decision variables for adding Cropping2D layers in decoder layers\n",
        "\n",
        "topcrop_after_upsampling1 = (math.ceil(math.ceil(IMAGE_HEIGHT/2)/2) % 2 != 0)\n",
        "leftcrop_after_upsampling1 = (math.ceil(math.ceil(IMAGE_WIDTH/2)/2) % 2 != 0)\n",
        "topcrop_after_upsampling2 = (math.ceil(IMAGE_HEIGHT/2) % 2 != 0)\n",
        "leftcrop_after_upsampling2 = (math.ceil(IMAGE_WIDTH/2) % 2 != 0)\n",
        "topcrop_after_upsampling3 = (IMAGE_HEIGHT % 2 != 0)\n",
        "leftcrop_after_upsampling3 = (IMAGE_WIDTH % 2 != 0)\n",
        "\n",
        "\n",
        "def decoder_layers(inputs, conv_shape, topcrop_after_upsampling1, \n",
        "                   leftcrop_after_upsampling1, topcrop_after_upsampling2, \n",
        "                   leftcrop_after_upsampling2,\n",
        "                   topcrop_after_upsampling3, leftcrop_after_upsampling3):\n",
        "    \"\"\"Defines the decoder layers.\n",
        "    Args:\n",
        "      inputs -- output of the encoder\n",
        "      conv_shape -- shape of the features before flattening\n",
        "\n",
        "    Returns:\n",
        "      tensor containing the decoded output\n",
        "    \"\"\"\n",
        "\n",
        "    # feed to a Dense network with units computed from the conv_shape dimensions\n",
        "    units = conv_shape[1] * conv_shape[2] * conv_shape[3]\n",
        "    x = tf.keras.layers.Dense(units, activation='relu', name=\"decode_dense1\")(inputs)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    # reshape output using the conv_shape dimensions\n",
        "    x = tf.keras.layers.Reshape((conv_shape[1], conv_shape[2], conv_shape[3]), \n",
        "                                name=\"decode_reshape\")(x)\n",
        "\n",
        "    # upsample the features back to the original dimensions\n",
        "    # for that, make sure to add Cropping2D layers after upsampling when needed\n",
        "    x = tf.keras.layers.Conv2DTranspose(filters=128, kernel_size=3, strides=2, \n",
        "                                        padding='same', activation='relu',\n",
        "                                        name=\"decode_conv2d_1\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    if topcrop_after_upsampling1:\n",
        "        x = tf.keras.layers.Cropping2D(cropping=((1, 0), (0, 0)))(x)\n",
        "    if leftcrop_after_upsampling1:\n",
        "        x = tf.keras.layers.Cropping2D(cropping=((0, 0), (1, 0)))(x)\n",
        "    x = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=2, \n",
        "                                        padding='same', activation='relu',\n",
        "                                        name=\"decode_conv2d_2\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    if topcrop_after_upsampling2:\n",
        "        x = tf.keras.layers.Cropping2D(cropping=((1, 0), (0, 0)))(x)\n",
        "    if leftcrop_after_upsampling2:\n",
        "        x = tf.keras.layers.Cropping2D(cropping=((0, 0), (1, 0)))(x)\n",
        "    x = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=2, \n",
        "                                        padding='same', activation='relu',\n",
        "                                        name=\"decode_conv2d_3\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    if topcrop_after_upsampling3:\n",
        "        x = tf.keras.layers.Cropping2D(cropping=((1, 0), (0, 0)))(x)\n",
        "    if leftcrop_after_upsampling3:\n",
        "        x = tf.keras.layers.Cropping2D(cropping=((0, 0), (1, 0)))(x)\n",
        "    x = tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=1, \n",
        "                                        padding='same', activation='sigmoid',\n",
        "                                        name=\"decode_final\")(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def decoder_model(latent_dim, conv_shape):\n",
        "    \"\"\"Defines the decoder model.\n",
        "    Args:\n",
        "      latent_dim -- dimensionality of the latent space\n",
        "      conv_shape -- shape of the features before flattening\n",
        "\n",
        "    Returns:\n",
        "      model -- the decoder model\n",
        "    \"\"\"\n",
        "\n",
        "    # set the inputs to the shape of the latent space\n",
        "    inputs = tf.keras.layers.Input(shape=(latent_dim,))\n",
        "\n",
        "    # get the output of the decoder layers\n",
        "    outputs = decoder_layers(inputs, conv_shape, topcrop_after_upsampling1, \n",
        "                             leftcrop_after_upsampling1, \n",
        "                             topcrop_after_upsampling2, \n",
        "                             leftcrop_after_upsampling2, \n",
        "                             topcrop_after_upsampling3, \n",
        "                             leftcrop_after_upsampling3)\n",
        "\n",
        "    # declare the inputs and outputs of the model\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "def kl_reconstruction_loss(inputs, outputs, mu, sigma):\n",
        "    \"\"\" Computes the Kullback-Leibler Divergence (KLD)\n",
        "    Args:\n",
        "      inputs -- batch from the dataset\n",
        "      outputs -- output of the Sampling layer\n",
        "      mu -- mean\n",
        "      sigma -- standard deviation\n",
        "\n",
        "    Returns:\n",
        "      KLD loss\n",
        "    \"\"\"\n",
        "    kl_loss = 1 + sigma - tf.square(mu) - tf.math.exp(sigma)\n",
        "    kl_loss = tf.reduce_mean(kl_loss) * -0.5\n",
        "\n",
        "    return kl_loss\n",
        "\n",
        "def vae_model(encoder, decoder, input_shape):\n",
        "    \"\"\"Defines the VAE model\n",
        "    Args:\n",
        "      encoder -- the encoder model\n",
        "      decoder -- the decoder model\n",
        "      input_shape -- shape of the dataset batch\n",
        "\n",
        "    Returns:\n",
        "      the complete VAE model\n",
        "    \"\"\"\n",
        "    # set the inputs\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "    # get mu, sigma, and z from the encoder output\n",
        "    mu, sigma, z = encoder(inputs)\n",
        "\n",
        "    # get reconstructed output from the decoder\n",
        "    reconstructed = decoder(z)\n",
        "\n",
        "    # define the inputs and outputs of the VAE\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=reconstructed)\n",
        "\n",
        "    # add the KL loss\n",
        "    loss = kl_reconstruction_loss(inputs, z, mu, sigma)\n",
        "    model.add_loss(loss)\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_models(input_shape, latent_dim):\n",
        "    \"\"\"Returns the encoder, decoder, and vae models\"\"\"\n",
        "    encoder, conv_shape = encoder_model(latent_dim=latent_dim, input_shape=input_shape)\n",
        "    decoder = decoder_model(latent_dim=latent_dim, conv_shape=conv_shape)\n",
        "    vae = vae_model(encoder, decoder, input_shape=input_shape)\n",
        "    return encoder, decoder, vae"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kuHjYojtmHz"
      },
      "source": [
        "# Define a VAE class via model subclassing\n",
        "loss_metrics = tf.keras.metrics.Mean()\n",
        "val_loss_metrics = tf.keras.metrics.Mean()\n",
        "\n",
        "class VAE(tf.keras.Model):\n",
        "    def __init__(self, encoder, decoder, variational_autoencoder):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.vae = variational_autoencoder\n",
        "\n",
        "    # override train_step method\n",
        "    def train_step(self, images):\n",
        "        if isinstance(images, tuple):\n",
        "            images = images[0]\n",
        "        with tf.GradientTape() as tape:\n",
        "            # feed a batch to the VAE model\n",
        "            reconstructed = self.vae(images)\n",
        "            # compute reconstruction loss\n",
        "            flattened_inputs = tf.reshape(images, [-1])\n",
        "            flattened_outputs = tf.reshape(reconstructed, [-1])\n",
        "            loss = self.compiled_loss(flattened_inputs, flattened_outputs) \\\n",
        "                   * images.shape[1] * images.shape[2]\n",
        "            # add KLD regularization loss\n",
        "            loss += sum(self.vae.losses)\n",
        "\n",
        "        # compute the gradients and update the model weights\n",
        "        grads = tape.gradient(loss, self.vae.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.vae.trainable_weights))\n",
        "\n",
        "        # update metrics\n",
        "        loss_metrics.update_state(loss)\n",
        "        \n",
        "        # return a dict mapping metrics names to current value\n",
        "        return {'loss': loss_metrics.result()}\n",
        "\n",
        "    # override test_step method\n",
        "    def test_step(self, images):\n",
        "        if isinstance(images, tuple):\n",
        "            images = images[0]\n",
        "        # compute predictions\n",
        "        reconstructed = self.vae(images)\n",
        "        # compute loss\n",
        "        flattened_inputs = tf.reshape(images, [-1])\n",
        "        flattened_outputs = tf.reshape(reconstructed, [-1])\n",
        "        loss = self.compiled_loss(flattened_inputs, flattened_outputs) \\\n",
        "               * images.shape[1] * images.shape[2]\n",
        "        # add KLD regularization loss\n",
        "        loss += sum(self.vae.losses)\n",
        "        # update metrics\n",
        "        val_loss_metrics.update_state(loss)\n",
        "        # return a dict mapping metrics names to current value\n",
        "        return {'loss': val_loss_metrics.result()}\n",
        "\n",
        "    def call(self, images):\n",
        "        if isinstance(images, tuple):\n",
        "            images = images[0]\n",
        "        return self.vae(images)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqBdfidbt3aF"
      },
      "source": [
        "def generate_and_save_images(model, epoch, step, test_input, vae_model):\n",
        "    \"\"\"Helper function to plot our 8 images\n",
        "\n",
        "    Args:\n",
        "\n",
        "    model -- the decoder model\n",
        "    epoch -- current epoch number during training\n",
        "    step -- current step number during training\n",
        "    test_input -- random tensor with shape (8, LATENT_DIM)\n",
        "    \"\"\"\n",
        "\n",
        "    # generate images from the test input\n",
        "    predictions = model.predict(test_input)\n",
        "\n",
        "    # plot the results\n",
        "    fig = plt.figure(figsize=(12, 14))\n",
        "\n",
        "    for i in range(predictions.shape[0]):\n",
        "        plt.subplot(8, 1, i + 1)\n",
        "        plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
        "        plt.axis('off')\n",
        "\n",
        "    # tight_layout minimizes the overlap between 2 sub-plots\n",
        "    fig.suptitle(\"epoch: {}, step: {}\".format(epoch, step))\n",
        "    plt.savefig('image_at_epoch_{:04d}_step{:04d}.png'.format(epoch, step))\n",
        "\n",
        "    if epoch != 0:\n",
        "        fig = plt.figure()\n",
        "        plt.scatter(range(len(vae_model.history.history['loss'])), \n",
        "                    vae_model.history.history['loss'])\n",
        "        plt.scatter(range(len(vae_model.history.history['val_loss'])), \n",
        "                    vae_model.history.history['val_loss'], color='red')\n",
        "        plt.savefig('Losses.png')\n",
        "    plt.show()\n",
        "\n",
        "def show_original_reconstructed_images(model, train_dataset):\n",
        "    plt.figure(figsize=(10, 14))\n",
        "    for input_images, _ in train_dataset:\n",
        "        k = 0\n",
        "        for i in range(5):\n",
        "            reconstructed = model(np.expand_dims(input_images[i], axis=0))\n",
        "            #reconstructed_categorized = np.where(reconstructed >= 0.5, 1, 0)\n",
        "            plt.subplot(5, 2, k+1)\n",
        "            plt.imshow(np.squeeze(input_images[i]), cmap='gray')\n",
        "            plt.subplot(5, 2, k+2)\n",
        "            #plt.imshow(np.squeeze(reconstructed_categorized), cmap='gray')\n",
        "            plt.imshow(np.squeeze(reconstructed), cmap='gray')\n",
        "            k += 2\n",
        "        plt.savefig(\"reconstructed_images.png\")\n",
        "    plt.show()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaZthzIQcdMP"
      },
      "source": [
        "Setting a learning rate scheduler for selecting the learning rate parameter during training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwJjP5p7t7Ob"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load and prepare image dataset for training\n",
        "#augmented_train_dataset, train_dataset, val_dataset, num_examples = get_datasets(test_size=TEST_SIZE)\n",
        "image_filenames = os.listdir(SOURCE_DIR)\n",
        "num_examples = len(image_filenames)\n",
        "\n",
        "# Create a callback that saves the model's weights every few epochs during training\n",
        "checkpoint_path = 'checkpoint/cp-cp{epoch:04d}.ckpt'\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_path,\n",
        "    verbose = 1,\n",
        "    save_weights_only = True,\n",
        "    save_freq = math.ceil(num_examples/BATCH_SIZE) * 100\n",
        ")\n",
        "\n",
        "# Create custom callback to display outputs (via helper function) at the end of each epoch of training\n",
        "class CustomCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        # Generate random vector as test input to the decoder\n",
        "        random_vector_for_generation = tf.random.normal(shape=[8, LATENT_DIM])\n",
        "        # Generate and save images\n",
        "        display.clear_output(wait=False)\n",
        "        if epoch % 100 == 0:\n",
        "            generate_and_save_images(decoder, epoch, \n",
        "                                     math.ceil(num_examples/BATCH_SIZE), \n",
        "                                     random_vector_for_generation, vae)\n",
        "        print('End of epoch {} - mean loss = {}'.format(epoch, logs[keys[0]]))\n",
        "\n",
        "# Create callback for adjusting learning rate during training\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-6 * 10**(epoch / 20))\n",
        "\n",
        "# Get the encoder, decoder and 'master' model (called vae)\n",
        "encoder, decoder, var_autoencoder = get_models(\n",
        "    input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS,), \n",
        "    latent_dim=LATENT_DIM)\n",
        "\n",
        "# Instantiate VAE class\n",
        "vae = VAE(encoder, decoder, var_autoencoder)\n",
        "\n",
        "# Compile model\n",
        "vae.compile(\n",
        "    optimizer = tf.keras.optimizers.Adam(lr=1e-6),\n",
        "    loss = tf.keras.losses.BinaryCrossentropy()\n",
        ")\n",
        "\n",
        "# Generate random vector as test input to the decoder\n",
        "random_vector_for_generation = tf.random.normal(shape=[8, LATENT_DIM])\n",
        "\n",
        "# Initialize the helper function to display outputs from an untrained model\n",
        "generate_and_save_images(decoder, 0, 0, random_vector_for_generation, vae)\n",
        "\n",
        "# Training loop using original dataset\n",
        "history = vae.fit(train_generator, epochs=100, verbose=1, \n",
        "                  callbacks=[cp_callback, CustomCallback(), lr_schedule])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BSZVxR26H6D"
      },
      "source": [
        "# Plot losses against learning rates\n",
        "plt.semilogx(vae.history.history['lr'], vae.history.history['loss'])\n",
        "plt.axis([1e-6, 0.01, 14500, 17400])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Loh9fstKcjT-"
      },
      "source": [
        "Training the model using the chosen learning rate value "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ja8NQJWpInIp"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load and prepare image dataset for training\n",
        "augmented_train_dataset, train_dataset, val_dataset, num_examples = get_datasets(test_size=TEST_SIZE)\n",
        "print(f\"Num of original examples: {num_examples}\")\n",
        "\n",
        "# Create a callback that saves the model's weights every few epochs during training\n",
        "checkpoint_path = './cp-cp{epoch:04d}.ckpt'\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_path,\n",
        "    verbose = 1,\n",
        "    save_weights_only = True,\n",
        "    save_freq = math.ceil(num_examples*(1-TEST_SIZE)/BATCH_SIZE) * 100\n",
        ")\n",
        "\n",
        "# Create custom callback to display outputs (via helper function) at the end of each epoch of training\n",
        "class CustomCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        # Generate random vector as test input to the decoder\n",
        "        random_vector_for_generation = tf.random.normal(shape=[8, LATENT_DIM])\n",
        "        # Generate and save images\n",
        "        print('End of epoch {} - mean loss = {}'.format(epoch, logs[keys[0]]))\n",
        "        display.clear_output(wait=False)\n",
        "        if epoch % 100 == 0:\n",
        "            generate_and_save_images(decoder, epoch, \n",
        "                                     math.ceil(num_examples/BATCH_SIZE), \n",
        "                                     random_vector_for_generation, vae)\n",
        "\n",
        "# Get the encoder, decoder and 'master' model (called vae)\n",
        "encoder, decoder, var_autoencoder = get_models(\n",
        "    input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS,), \n",
        "    latent_dim=LATENT_DIM)\n",
        "\n",
        "# Instantiate VAE class\n",
        "vae = VAE(encoder, decoder, var_autoencoder)\n",
        "\n",
        "# Compile model\n",
        "vae.compile(\n",
        "    optimizer = tf.keras.optimizers.Adam(lr=8e-4),\n",
        "    loss = tf.keras.losses.BinaryCrossentropy()\n",
        ")\n",
        "\n",
        "# Generate random vector as test input to the decoder\n",
        "random_vector_for_generation = tf.random.normal(shape=[8, LATENT_DIM])\n",
        "\n",
        "# Initialize the helper function to display outputs from an untrained model\n",
        "generate_and_save_images(decoder, 0, 0, random_vector_for_generation, vae)\n",
        "\n",
        "# Training loop using original dataset\n",
        "history = vae.fit(x=augmented_train_dataset, \n",
        "                  epochs=EPOCHS,  \n",
        "                  verbose=1, \n",
        "                  validation_data = val_dataset, \n",
        "                  callbacks=[cp_callback, CustomCallback()])      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMTLorbeoCNu"
      },
      "source": [
        "plt.scatter(range(len(vae.history.history['loss'])), vae.history.history['loss'])\n",
        "plt.scatter(range(len(vae.history.history['val_loss'])), vae.history.history['val_loss'], color='red')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6sQRON4qRYB"
      },
      "source": [
        "# Show reconstructed images\n",
        "show_original_reconstructed_images(vae, train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffh7_Sr2cqp6"
      },
      "source": [
        "Instantiate new model, load weights from last checkpoint and resume training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBUrnwpMuCTv"
      },
      "source": [
        "# Get the encoder, decoder and 'master' model (called vae)\n",
        "encoder, decoder, var_autoencoder = get_models(input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS,), latent_dim=LATENT_DIM)\n",
        "\n",
        "# Create new instance of VAE class\n",
        "new_vae = VAE(encoder, decoder, var_autoencoder)\n",
        "\n",
        "# Load weights from last checkpoint\n",
        "checkpoint_dir = '.'\n",
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "print(latest)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7liWl_S9Wzzh"
      },
      "source": [
        "new_vae.load_weights(latest)\n",
        "new_vae.compile(\n",
        "    optimizer = tf.keras.optimizers.Adam(8e-4),\n",
        "    loss = tf.keras.losses.BinaryCrossentropy()\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbi_f_RFvE3T"
      },
      "source": [
        "new_vae.evaluate(augmented_train_dataset, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veT2g3x7u_0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7b67c2df-246e-4af7-ec6a-10571f1581cc"
      },
      "source": [
        "# Create a callback that saves the model's weights every few epochs during training\n",
        "checkpoint_path = './cp-cp{epoch:04d}.ckpt'\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_path,\n",
        "    verbose = 1,\n",
        "    save_weights_only = True,\n",
        "    save_freq = math.ceil(num_examples*(1-TEST_SIZE)/BATCH_SIZE) * 100\n",
        ")\n",
        "# Create custom callback to display outputs (via helper function) at the end of each epoch of training\n",
        "class CustomCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        # Generate random vector as test input to the decoder\n",
        "        random_vector_for_generation = tf.random.normal(shape=[8, LATENT_DIM])\n",
        "        # Generate and save images\n",
        "        display.clear_output(wait=True)\n",
        "        if epoch % 100 == 0:\n",
        "            generate_and_save_images(decoder, epoch, \n",
        "                                     math.ceil(num_examples/BATCH_SIZE), \n",
        "                                     random_vector_for_generation, new_vae)\n",
        "        print('End of epoch {} - mean loss = {}'.format(epoch, logs[keys[0]]))\n",
        "\n",
        "# Resume training using original dataset\n",
        "new_vae.fit(x=augmented_train_dataset, epochs=800, \n",
        "            verbose=1, \n",
        "            validation_data=val_dataset, \n",
        "            callbacks=[cp_callback, CustomCallback()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "End of epoch 799 - mean loss = 12248.623046875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f198d7dfe10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1tikmPh6yYh"
      },
      "source": [
        "plt.scatter(range(len(new_vae.history.history['loss'])), new_vae.history.history['loss'])\n",
        "plt.scatter(range(len(new_vae.history.history['loss'])), new_vae.history.history['val_loss'], color='red')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDT7Jm4ndBfB"
      },
      "source": [
        "Compare the original images with the reconstructed images from the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVLen6k1uIJ-"
      },
      "source": [
        "# Show reconstructed images\n",
        "augmented_train_dataset, train_dataset, val_dataset, num_examples = get_datasets(test_size=TEST_SIZE)\n",
        "show_original_reconstructed_images(new_vae, train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
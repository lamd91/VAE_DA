{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "VAE_MPS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lamd91/VAE_DA/blob/master/VAE_MPS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_8YcIzsMhdd"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "import random\n",
        "from IPython import display\n",
        "import os\n",
        "import shutil\n",
        "from PIL import Image as im"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlmkMoV9r0Zu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24d72948-f15e-4384-9285-3d082d1a96ef"
      },
      "source": [
        "# Mount Google Drive at \"/content/gdrive\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vbMSVpOr8oS"
      },
      "source": [
        "After having uploaded `mps_simulations.tar.gz` located in the \"data\" folder of the github repository, extract the folder content here in Colab. The extracted files will be located in a directory named `images`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbWdJwWQHnwc"
      },
      "source": [
        "!tar -xf \"/content/gdrive/My Drive/mps_simulations.tar.gz\" \n",
        "!rm -r images/.ipynb_checkpoints\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "i-JwifzONA43",
        "outputId": "10c4f449-e893-48e1-851b-bfef3b9ad394"
      },
      "source": [
        "# Show one example of MPS simulation that will be used for training\n",
        "i = random.randint(0, 1999)\n",
        "image = pickle.load(open(f'images/img-{i}.pickle', 'rb')).astype('uint8')\n",
        "plt.imshow(image)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fdcfbe0e150>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nejBn076s2So",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9f0c591-4401-4fe8-f0b6-41c50b820612"
      },
      "source": [
        "# Define global constants\n",
        "BATCH_SIZE = 32\n",
        "LATENT_DIM = 2\n",
        "EPOCHS = 800\n",
        "IMAGE_HEIGHT = 50\n",
        "IMAGE_WIDTH = 500\n",
        "NUM_CHANNELS = 1\n",
        "TEST_SIZE = 0.2\n",
        "SOURCE_DIR = 'images'\n",
        "TRAINING_DIR = 'training_images'\n",
        "TESTING_DIR = 'testing_images' \n",
        "\n",
        "# Other variables\n",
        "num_examples = len(os.listdir(SOURCE_DIR))\n",
        "print(\"Number of examples:\", num_examples)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of examples: 2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpcXzUL6XSwp",
        "outputId": "a35e505c-cab3-4fe4-8721-20041fcbc46b"
      },
      "source": [
        "random.seed(42)\n",
        "\n",
        "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
        "    \"\"\"Split images into training and testing directories\"\"\"\n",
        "\n",
        "    image_filenames = os.listdir(SOURCE)\n",
        "    num_examples = len(image_filenames)\n",
        "    if not os.path.exists(os.path.join(TRAINING, 'input_images')):\n",
        "        os.makedirs(os.path.join(TRAINING, 'input_images'))\n",
        "    if not os.path.exists(os.path.join(TESTING, 'input_images')):\n",
        "        os.makedirs(os.path.join(TESTING, 'input_images'))\n",
        "\n",
        "    count = 0\n",
        "    num_training_examples = int(num_examples * (1-SPLIT_SIZE))\n",
        "    for file in random.sample(image_filenames, num_examples):\n",
        "        src = os.path.join(SOURCE, file)\n",
        "        if os.path.getsize(src) != 0:\n",
        "            if count < num_training_examples:\n",
        "                array = pickle.load(open(src, 'rb'))\n",
        "                img = im.fromarray(array.astype('uint8')*255, 'L')\n",
        "                img.save(os.path.join(TRAINING, 'input_images', file.replace('.pickle', '')) + '.png')\n",
        "            else:\n",
        "                array = pickle.load(open(src, 'rb'))\n",
        "                img = im.fromarray(array.astype('uint8')*255, 'L')\n",
        "                img.save(os.path.join(TESTING, 'input_images', file.replace('.pickle', '')) + '.png')\n",
        "            count += 1\n",
        "\n",
        "    print(\"Number of training examples:\", int(len(os.listdir(os.path.join(TRAINING, 'input_images')))))\n",
        "    print(\"Number of validation examples:\", int(len(os.listdir(os.path.join(TESTING, 'input_images')))))\n",
        "\n",
        "split_data(SOURCE_DIR, TRAINING_DIR, TESTING_DIR, TEST_SIZE)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 1600\n",
            "Number of validation examples: 400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klRQAqMks5PD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fab060a6-d73a-4099-d875-796b95d2b54e"
      },
      "source": [
        "def create_data_generators():\n",
        "    \"\"\"Create generators of training data and validation data\"\"\" \n",
        "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        rescale = 1.0/255.0,\n",
        "        width_shift_range = 0.15,\n",
        "        horizontal_flip = True,\n",
        "        vertical_flip = True,\n",
        "        fill_mode='nearest'    \n",
        "    )\n",
        "\n",
        "    val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        rescale = 1.0/255.0,\n",
        "    )\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        directory = TRAINING_DIR,\n",
        "        batch_size = BATCH_SIZE,\n",
        "        target_size = (50, 500),\n",
        "        color_mode = 'grayscale', # images will be converted to have 1 channel\n",
        "        class_mode = 'input', # labels will be images identical to input images\n",
        "    )\n",
        "\n",
        "    val_generator = val_datagen.flow_from_directory(\n",
        "        directory = TESTING_DIR,\n",
        "        batch_size = BATCH_SIZE,\n",
        "        target_size = (50, 500),\n",
        "        color_mode = 'grayscale',\n",
        "        class_mode = 'input'\n",
        "    )\n",
        "\n",
        "    return train_generator, val_generator\n",
        "\n",
        "train_generator, val_generator = create_data_generators()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1600 images belonging to 1 classes.\n",
            "Found 400 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZReHg3QTtfe9"
      },
      "source": [
        "class Sampling(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Generates a random sample and combines with the encoder output\n",
        "\n",
        "        Args:\n",
        "          inputs -- output tensor from the encoder\n",
        "\n",
        "        Returns:\n",
        "          `inputs` tensors combined with a random sample\n",
        "        \"\"\"\n",
        "\n",
        "        # unpack the output of the encoder\n",
        "        mu, sigma = inputs\n",
        "\n",
        "        # get the size and dimensions of the batch\n",
        "        batch = tf.shape(mu)[0]\n",
        "        dim = tf.shape(mu)[1]\n",
        "\n",
        "        # generate a random tensor\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "\n",
        "        # combine the inputs and noise\n",
        "        return mu + tf.exp(0.5 * sigma) * epsilon\n",
        "\n",
        "\n",
        "def encoder_layers(inputs, latent_dim):\n",
        "    \"\"\"Defines the encoder's layers.\n",
        "    Args:\n",
        "      inputs -- batch from the dataset\n",
        "      latent_dim -- dimensionality of the latent space\n",
        "\n",
        "    Returns:\n",
        "      mu -- learned mean\n",
        "      sigma -- learned standard deviation\n",
        "      batch_2.shape -- shape of the features before flattening\n",
        "    \"\"\"\n",
        "\n",
        "    # add the Conv2D layers followed by BatchNormalization\n",
        "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, \n",
        "                               padding=\"same\", activation='relu',\n",
        "                               name=\"encode_conv1\")(inputs)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, \n",
        "                               padding='same', activation='relu',\n",
        "                               name=\"encode_conv2\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=2, \n",
        "                               padding='same', activation='relu',\n",
        "                               name=\"encode_conv3\")(x)\n",
        "                               \n",
        "    # assign to a different variable so you can extract the shape later\n",
        "    batch_3 = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    # flatten the features and feed into the Dense network\n",
        "    x = tf.keras.layers.Flatten(name=\"encode_flatten\")(batch_3)\n",
        "\n",
        "    # we arbitrarily used ... units here but feel free to change\n",
        "    x = tf.keras.layers.Dense(1024, activation='relu', name=\"encode_dense\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    # add output Dense networks for mu and sigma, units equal to the declared latent_dim.\n",
        "    mu = tf.keras.layers.Dense(latent_dim, name='latent_mu')(x)\n",
        "    sigma = tf.keras.layers.Dense(latent_dim, name='latent_sigma')(x)\n",
        "\n",
        "    return mu, sigma, batch_3.shape\n",
        "\n",
        "def encoder_model(latent_dim, input_shape):\n",
        "    \"\"\"Defines the encoder model with the Sampling layer\n",
        "    Args:\n",
        "      latent_dim -- dimensionality of the latent space\n",
        "      input_shape -- shape of the dataset batch\n",
        "\n",
        "    Returns:\n",
        "      model -- the encoder model\n",
        "      conv_shape -- shape of the features before flattening\n",
        "    \"\"\"\n",
        "\n",
        "    # declare the inputs tensor with the given shape\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "    # get the output of the encoder_layers() function\n",
        "    mu, sigma, conv_shape = encoder_layers(inputs, latent_dim=LATENT_DIM)\n",
        "\n",
        "    # feed mu and sigma to the Sampling layer\n",
        "    z = Sampling()((mu, sigma))\n",
        "\n",
        "    # build the whole encoder model\n",
        "    model = tf.keras.Model(inputs, outputs=[mu, sigma, z])\n",
        "\n",
        "    return model, conv_shape\n",
        "\n",
        "\n",
        "# Define decision variables for adding Cropping2D layers in decoder layers\n",
        "\n",
        "topcrop_after_upsampling1 = (math.ceil(math.ceil(IMAGE_HEIGHT/2)/2) % 2 != 0)\n",
        "leftcrop_after_upsampling1 = (math.ceil(math.ceil(IMAGE_WIDTH/2)/2) % 2 != 0)\n",
        "topcrop_after_upsampling2 = (math.ceil(IMAGE_HEIGHT/2) % 2 != 0)\n",
        "leftcrop_after_upsampling2 = (math.ceil(IMAGE_WIDTH/2) % 2 != 0)\n",
        "topcrop_after_upsampling3 = (IMAGE_HEIGHT % 2 != 0)\n",
        "leftcrop_after_upsampling3 = (IMAGE_WIDTH % 2 != 0)\n",
        "\n",
        "\n",
        "def decoder_layers(inputs, conv_shape, topcrop_after_upsampling1, \n",
        "                   leftcrop_after_upsampling1, topcrop_after_upsampling2, \n",
        "                   leftcrop_after_upsampling2,\n",
        "                   topcrop_after_upsampling3, leftcrop_after_upsampling3):\n",
        "    \"\"\"Defines the decoder layers.\n",
        "    Args:\n",
        "      inputs -- output of the encoder\n",
        "      conv_shape -- shape of the features before flattening\n",
        "\n",
        "    Returns:\n",
        "      tensor containing the decoded output\n",
        "    \"\"\"\n",
        "\n",
        "    # feed to a Dense network with units computed from the conv_shape dimensions\n",
        "    units = conv_shape[1] * conv_shape[2] * conv_shape[3]\n",
        "    x = tf.keras.layers.Dense(units, activation='relu', name=\"decode_dense1\")(inputs)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    # reshape output using the conv_shape dimensions\n",
        "    x = tf.keras.layers.Reshape((conv_shape[1], conv_shape[2], conv_shape[3]), \n",
        "                                name=\"decode_reshape\")(x)\n",
        "\n",
        "    # upsample the features back to the original dimensions\n",
        "    # for that, make sure to add Cropping2D layers after upsampling when needed\n",
        "    x = tf.keras.layers.Conv2DTranspose(filters=128, kernel_size=3, strides=2, \n",
        "                                        padding='same', activation='relu',\n",
        "                                        name=\"decode_conv2d_1\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    if topcrop_after_upsampling1:\n",
        "        x = tf.keras.layers.Cropping2D(cropping=((1, 0), (0, 0)))(x)\n",
        "    if leftcrop_after_upsampling1:\n",
        "        x = tf.keras.layers.Cropping2D(cropping=((0, 0), (1, 0)))(x)\n",
        "    x = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=2, \n",
        "                                        padding='same', activation='relu',\n",
        "                                        name=\"decode_conv2d_2\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    if topcrop_after_upsampling2:\n",
        "        x = tf.keras.layers.Cropping2D(cropping=((1, 0), (0, 0)))(x)\n",
        "    if leftcrop_after_upsampling2:\n",
        "        x = tf.keras.layers.Cropping2D(cropping=((0, 0), (1, 0)))(x)\n",
        "    x = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=2, \n",
        "                                        padding='same', activation='relu',\n",
        "                                        name=\"decode_conv2d_3\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    if topcrop_after_upsampling3:\n",
        "        x = tf.keras.layers.Cropping2D(cropping=((1, 0), (0, 0)))(x)\n",
        "    if leftcrop_after_upsampling3:\n",
        "        x = tf.keras.layers.Cropping2D(cropping=((0, 0), (1, 0)))(x)\n",
        "    x = tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=1, \n",
        "                                        padding='same', activation='sigmoid',\n",
        "                                        name=\"decode_final\")(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def decoder_model(latent_dim, conv_shape):\n",
        "    \"\"\"Defines the decoder model.\n",
        "    Args:\n",
        "      latent_dim -- dimensionality of the latent space\n",
        "      conv_shape -- shape of the features before flattening\n",
        "\n",
        "    Returns:\n",
        "      model -- the decoder model\n",
        "    \"\"\"\n",
        "\n",
        "    # set the inputs to the shape of the latent space\n",
        "    inputs = tf.keras.layers.Input(shape=(latent_dim,))\n",
        "\n",
        "    # get the output of the decoder layers\n",
        "    outputs = decoder_layers(inputs, conv_shape, topcrop_after_upsampling1, \n",
        "                             leftcrop_after_upsampling1, \n",
        "                             topcrop_after_upsampling2, \n",
        "                             leftcrop_after_upsampling2, \n",
        "                             topcrop_after_upsampling3, \n",
        "                             leftcrop_after_upsampling3)\n",
        "\n",
        "    # declare the inputs and outputs of the model\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "def kl_reconstruction_loss(inputs, outputs, mu, sigma):\n",
        "    \"\"\" Computes the Kullback-Leibler Divergence (KLD)\n",
        "    Args:\n",
        "      inputs -- batch from the dataset\n",
        "      outputs -- output of the Sampling layer\n",
        "      mu -- mean\n",
        "      sigma -- standard deviation\n",
        "\n",
        "    Returns:\n",
        "      KLD loss\n",
        "    \"\"\"\n",
        "    kl_loss = 1 + sigma - tf.square(mu) - tf.math.exp(sigma)\n",
        "    kl_loss = tf.reduce_mean(kl_loss) * -0.5\n",
        "\n",
        "    return kl_loss\n",
        "\n",
        "def vae_model(encoder, decoder, input_shape):\n",
        "    \"\"\"Defines the VAE model\n",
        "    Args:\n",
        "      encoder -- the encoder model\n",
        "      decoder -- the decoder model\n",
        "      input_shape -- shape of the dataset batch\n",
        "\n",
        "    Returns:\n",
        "      the complete VAE model\n",
        "    \"\"\"\n",
        "    # set the inputs\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "    # get mu, sigma, and z from the encoder output\n",
        "    mu, sigma, z = encoder(inputs)\n",
        "\n",
        "    # get reconstructed output from the decoder\n",
        "    reconstructed = decoder(z)\n",
        "\n",
        "    # define the inputs and outputs of the VAE\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=reconstructed)\n",
        "\n",
        "    # add the KL loss\n",
        "    loss = kl_reconstruction_loss(inputs, z, mu, sigma)\n",
        "    model.add_loss(loss)\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_models(input_shape, latent_dim):\n",
        "    \"\"\"Returns the encoder, decoder, and vae models\"\"\"\n",
        "    encoder, conv_shape = encoder_model(latent_dim=latent_dim, input_shape=input_shape)\n",
        "    decoder = decoder_model(latent_dim=latent_dim, conv_shape=conv_shape)\n",
        "    vae = vae_model(encoder, decoder, input_shape=input_shape)\n",
        "    return encoder, decoder, vae"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kuHjYojtmHz"
      },
      "source": [
        "# Define a VAE class via model subclassing\n",
        "loss_metrics = tf.keras.metrics.Mean()\n",
        "val_loss_metrics = tf.keras.metrics.Mean()\n",
        "\n",
        "class VAE(tf.keras.Model):\n",
        "    def __init__(self, encoder, decoder, variational_autoencoder):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.vae = variational_autoencoder\n",
        "\n",
        "    # override train_step method\n",
        "    def train_step(self, images):\n",
        "        if isinstance(images, tuple):\n",
        "            images = images[0]\n",
        "        with tf.GradientTape() as tape:\n",
        "            # feed a batch to the VAE model\n",
        "            reconstructed = self.vae(images)\n",
        "            # compute reconstruction loss\n",
        "            flattened_inputs = tf.reshape(images, [-1])\n",
        "            flattened_outputs = tf.reshape(reconstructed, [-1])\n",
        "            loss = self.compiled_loss(flattened_inputs, flattened_outputs) \\\n",
        "                   * images.shape[1] * images.shape[2]\n",
        "            # add KLD regularization loss\n",
        "            loss += sum(self.vae.losses)\n",
        "\n",
        "        # compute the gradients and update the model weights\n",
        "        grads = tape.gradient(loss, self.vae.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.vae.trainable_weights))\n",
        "\n",
        "        # update metrics\n",
        "        loss_metrics.update_state(loss)\n",
        "        \n",
        "        # return a dict mapping metrics names to current value\n",
        "        return {'loss': loss_metrics.result()}\n",
        "\n",
        "    # override test_step method\n",
        "    def test_step(self, images):\n",
        "        if isinstance(images, tuple):\n",
        "            images = images[0]\n",
        "        # compute predictions\n",
        "        reconstructed = self.vae(images)\n",
        "        # compute loss\n",
        "        flattened_inputs = tf.reshape(images, [-1])\n",
        "        flattened_outputs = tf.reshape(reconstructed, [-1])\n",
        "        loss = self.compiled_loss(flattened_inputs, flattened_outputs) \\\n",
        "               * images.shape[1] * images.shape[2]\n",
        "        # add KLD regularization loss\n",
        "        loss += sum(self.vae.losses)\n",
        "        # update metrics\n",
        "        val_loss_metrics.update_state(loss)\n",
        "        # return a dict mapping metrics names to current value\n",
        "        return {'loss': val_loss_metrics.result()}\n",
        "\n",
        "    def call(self, images):\n",
        "        if isinstance(images, tuple):\n",
        "            images = images[0]\n",
        "        return self.vae(images)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqBdfidbt3aF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "5093ba0b-0b8d-4ecf-ecd7-3a6d947269a3"
      },
      "source": [
        "def generate_and_save_images(model, epoch, step, test_input, vae_model):\n",
        "    \"\"\"Helper function to plot our 8 images\n",
        "\n",
        "    Args:\n",
        "\n",
        "    model -- the decoder model\n",
        "    epoch -- current epoch number during training\n",
        "    step -- current step number during training\n",
        "    test_input -- random tensor with shape (8, LATENT_DIM)\n",
        "    \"\"\"\n",
        "\n",
        "    # generate images from the test input\n",
        "    predictions = model.predict(test_input)\n",
        "\n",
        "    # plot the results\n",
        "    fig = plt.figure(figsize=(12, 14))\n",
        "\n",
        "    for i in range(predictions.shape[0]):\n",
        "        plt.subplot(8, 1, i + 1)\n",
        "        plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
        "        plt.axis('off')\n",
        "\n",
        "    # tight_layout minimizes the overlap between 2 sub-plots\n",
        "    fig.suptitle(\"epoch: {}, step: {}\".format(epoch, step))\n",
        "    plt.savefig('image_at_epoch_{:04d}_step{:04d}.png'.format(epoch, step))\n",
        "\n",
        "    if epoch != 0:\n",
        "        fig = plt.figure()\n",
        "        plt.scatter(range(len(vae_model.history.history['loss'])), \n",
        "                    vae_model.history.history['loss'])\n",
        "        plt.scatter(range(len(vae_model.history.history['val_loss'])), \n",
        "                    vae_model.history.history['val_loss'], color='red')\n",
        "        plt.savefig('Losses.png')\n",
        "    plt.show()\n",
        "\n",
        "def show_original_reconstructed_images(model, generator):\n",
        "    plt.figure(figsize=(10, 14))\n",
        "    input_images, _ = generator.next()\n",
        "    for i in range(5):\n",
        "        reconstructed = model(np.expand_dims(input_images[i], axis=0))\n",
        "        #reconstructed_categorized = np.where(reconstructed >= 0.5, 1, 0)\n",
        "        plt.subplot(5, 2, k+1)\n",
        "        plt.imshow(np.squeeze(input_images[i]), cmap='gray')\n",
        "        plt.subplot(5, 2, k+2)\n",
        "        #plt.imshow(np.squeeze(reconstructed_categorized), cmap='gray')\n",
        "        plt.imshow(np.squeeze(reconstructed), cmap='gray')\n",
        "        k += 2\n",
        "      plt.savefig(\"reconstructed_images.png\")\n",
        "    plt.show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-101816a748a4>\"\u001b[0;36m, line \u001b[0;32m48\u001b[0m\n\u001b[0;31m    plt.savefig(\"reconstructed_images.png\")\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaZthzIQcdMP"
      },
      "source": [
        "Setting a learning rate scheduler for selecting the learning rate parameter during training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwJjP5p7t7Ob",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        },
        "outputId": "064f9303-30d9-499d-c493-c432856aaee2"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create a callback that saves the model's weights every few epochs during training\n",
        "checkpoint_path = 'checkpoint/cp-cp{epoch:04d}.ckpt'\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_path,\n",
        "    verbose = 1,\n",
        "    save_weights_only = True,\n",
        "    save_freq = math.ceil(num_examples/BATCH_SIZE) * 100\n",
        ")\n",
        "\n",
        "# Create custom callback to display outputs (via helper function) at the end of each epoch of training\n",
        "class CustomCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        # Generate random vector as test input to the decoder\n",
        "        random_vector_for_generation = tf.random.normal(shape=[8, LATENT_DIM])\n",
        "        # Generate and save images\n",
        "        display.clear_output(wait=False)\n",
        "        if epoch % 100 == 0:\n",
        "            generate_and_save_images(decoder, epoch, \n",
        "                                     math.ceil(num_examples/BATCH_SIZE), \n",
        "                                     random_vector_for_generation, vae)\n",
        "        print('End of epoch {} - mean loss = {}'.format(epoch, logs[keys[0]]))\n",
        "\n",
        "# Create callback for adjusting learning rate during training\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-6 * 10**(epoch / 20))\n",
        "\n",
        "# Get the encoder, decoder and 'master' model (called vae)\n",
        "encoder, decoder, var_autoencoder = get_models(\n",
        "    input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS,), \n",
        "    latent_dim=LATENT_DIM)\n",
        "\n",
        "# Instantiate VAE class\n",
        "vae = VAE(encoder, decoder, var_autoencoder)\n",
        "\n",
        "# Compile model\n",
        "vae.compile(\n",
        "    optimizer = tf.keras.optimizers.Adam(lr=1e-6),\n",
        "    loss = tf.keras.losses.BinaryCrossentropy()\n",
        ")\n",
        "\n",
        "# Generate random vector as test input to the decoder\n",
        "random_vector_for_generation = tf.random.normal(shape=[8, LATENT_DIM])\n",
        "\n",
        "# Initialize the helper function to display outputs from an untrained model\n",
        "generate_and_save_images(decoder, 0, 0, random_vector_for_generation, vae)\n",
        "\n",
        "# Training loop using original dataset\n",
        "history = vae.fit(train_generator, epochs=100, verbose=1, \n",
        "                  callbacks=[cp_callback, CustomCallback(), lr_schedule])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "End of epoch 99 - mean loss = nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BSZVxR26H6D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "8a0a900f-fd47-4b94-f727-ef60e2721d12"
      },
      "source": [
        "# Plot losses against learning rates\n",
        "plt.semilogx(vae.history.history['lr'], vae.history.history['loss'])\n",
        "plt.axis([1e-6, 0.01, 14000, 17500])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1e-06, 0.01, 14000.0, 17500.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Loh9fstKcjT-"
      },
      "source": [
        "Training the model using the chosen learning rate value "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ja8NQJWpInIp"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create a callback that saves the model's weights every few epochs during training\n",
        "checkpoint_path = './cp-cp{epoch:04d}.ckpt'\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_path,\n",
        "    verbose = 1,\n",
        "    save_weights_only = True,\n",
        "    save_freq = math.ceil(num_examples*(1-TEST_SIZE)/BATCH_SIZE) * 100\n",
        ")\n",
        "\n",
        "# Create custom callback to display outputs (via helper function) at the end of each epoch of training\n",
        "class CustomCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        # Generate random vector as test input to the decoder\n",
        "        random_vector_for_generation = tf.random.normal(shape=[8, LATENT_DIM])\n",
        "        # Generate and save images\n",
        "        print('End of epoch {} - mean loss = {}'.format(epoch, logs[keys[0]]))\n",
        "        display.clear_output(wait=False)\n",
        "        if epoch % 100 == 0:\n",
        "            generate_and_save_images(decoder, epoch, \n",
        "                                     math.ceil(num_examples/BATCH_SIZE), \n",
        "                                     random_vector_for_generation, vae)\n",
        "\n",
        "# Get the encoder, decoder and 'master' model (called vae)\n",
        "encoder, decoder, var_autoencoder = get_models(\n",
        "    input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS,), \n",
        "    latent_dim=LATENT_DIM)\n",
        "\n",
        "# Instantiate VAE class\n",
        "vae = VAE(encoder, decoder, var_autoencoder)\n",
        "\n",
        "# Compile model\n",
        "vae.compile(\n",
        "    optimizer = tf.keras.optimizers.Adam(lr=8e-4),\n",
        "    loss = tf.keras.losses.BinaryCrossentropy()\n",
        ")\n",
        "\n",
        "# Generate random vector as test input to the decoder\n",
        "random_vector_for_generation = tf.random.normal(shape=[8, LATENT_DIM])\n",
        "\n",
        "# Initialize the helper function to display outputs from an untrained model\n",
        "generate_and_save_images(decoder, 0, 0, random_vector_for_generation, vae)\n",
        "\n",
        "# Training loop using original dataset\n",
        "history = vae.fit(x=train_generator, \n",
        "                  epochs=EPOCHS,  \n",
        "                  verbose=1, \n",
        "                  validation_data = val_generator, \n",
        "                  callbacks=[cp_callback, CustomCallback()])      "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMTLorbeoCNu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "663c0026-a7c5-4258-ad1e-9aa920935b63"
      },
      "source": [
        "plt.scatter(range(len(vae.history.history['loss'])), vae.history.history['loss'])\n",
        "plt.scatter(range(len(vae.history.history['val_loss'])), vae.history.history['val_loss'], color='red')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7fdca6a3b390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR3TujeGbDzh"
      },
      "source": [
        "# Show reconstructed images from trained model\n",
        "show_original_reconstructed_images(vae, train_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffh7_Sr2cqp6"
      },
      "source": [
        "Instantiate new model, load weights from last checkpoint and resume training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBUrnwpMuCTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0bb5632-ddfe-4749-b415-2eb5c01a25fb"
      },
      "source": [
        "# Get the encoder, decoder and 'master' model (called vae)\n",
        "encoder, decoder, var_autoencoder = get_models(input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS,), latent_dim=LATENT_DIM)\n",
        "\n",
        "# Create new instance of VAE class\n",
        "new_vae = VAE(encoder, decoder, var_autoencoder)\n",
        "\n",
        "# Load weights from last checkpoint\n",
        "checkpoint_dir = '.'\n",
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "print(latest)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./cp-cp0800.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7liWl_S9Wzzh"
      },
      "source": [
        "new_vae.load_weights(latest)\n",
        "new_vae.compile(\n",
        "    optimizer = tf.keras.optimizers.Adam(8e-4),\n",
        "    loss = tf.keras.losses.BinaryCrossentropy()\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbi_f_RFvE3T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d532fdb5-ca1d-4d1f-d294-82e5ede65535"
      },
      "source": [
        "new_vae.evaluate(train_generator, verbose=1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50/50 [==============================] - 5s 88ms/step - loss: 12139.6022\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12137.267578125"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veT2g3x7u_0b"
      },
      "source": [
        "# Create a callback that saves the model's weights every few epochs during training\n",
        "checkpoint_path = './cp-cp{epoch:04d}.ckpt'\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_path,\n",
        "    verbose = 1,\n",
        "    save_weights_only = True,\n",
        "    save_freq = math.ceil(num_examples*(1-TEST_SIZE)/BATCH_SIZE) * 100\n",
        ")\n",
        "# Create custom callback to display outputs (via helper function) at the end of each epoch of training\n",
        "class CustomCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        # Generate random vector as test input to the decoder\n",
        "        random_vector_for_generation = tf.random.normal(shape=[8, LATENT_DIM])\n",
        "        # Generate and save images\n",
        "        display.clear_output(wait=True)\n",
        "        if epoch % 100 == 0:\n",
        "            generate_and_save_images(decoder, epoch, \n",
        "                                     math.ceil(num_examples/BATCH_SIZE), \n",
        "                                     random_vector_for_generation, new_vae)\n",
        "        print('End of epoch {} - mean loss = {}'.format(epoch, logs[keys[0]]))\n",
        "\n",
        "# Resume training using original dataset\n",
        "new_vae.fit(x=augmented_train_dataset, epochs=800, \n",
        "            verbose=1, \n",
        "            validation_data=val_dataset, \n",
        "            callbacks=[cp_callback, CustomCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1tikmPh6yYh"
      },
      "source": [
        "plt.scatter(range(len(new_vae.history.history['loss'])), new_vae.history.history['loss'])\n",
        "plt.scatter(range(len(new_vae.history.history['loss'])), new_vae.history.history['val_loss'], color='red')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDT7Jm4ndBfB"
      },
      "source": [
        "Compare the original images with the reconstructed images from the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVLen6k1uIJ-"
      },
      "source": [
        "# Show reconstructed images from trained model\n",
        "show_original_reconstructed_images(new_vae, train_generator)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
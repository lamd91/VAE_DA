{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "VAE_MPS_last.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lamd91/VAE_DA/blob/master/VAE_MPS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlmkMoV9r0Zu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af777d3e-d324-437c-8e94-e2d422c9bdd5"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "from IPython import display\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nejBn076s2So"
      },
      "source": [
        "# Define global constants\n",
        "BATCH_SIZE = 32\n",
        "LATENT_DIM = 2\n",
        "EPOCHS = 800\n",
        "IMAGE_HEIGHT = 50\n",
        "IMAGE_WIDTH = 500\n",
        "NUM_CHANNELS = 1\n",
        "TEST_SIZE = 0.2"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klRQAqMks5PD"
      },
      "source": [
        "def map_image(image):\n",
        "    '''Returns a reshaped tensor from a given image as input and as label'''\n",
        "    image = tf.cast(image, dtype=tf.float32)\n",
        "    image = tf.reshape(image, shape=(IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS,))\n",
        "\n",
        "    return image, image\n",
        "\n",
        "def augment_image(image):\n",
        "    '''Returns as input and as label a reshaped tensor from a given image \n",
        "    which may have undergone some horizontal or vertical flipping '''\n",
        "    image = tf.cast(image, dtype=tf.float32)\n",
        "    image = tf.reshape(image, shape=(IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS,))\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "\n",
        "    return image, image\n",
        "\n",
        "# def get_datasets(test_size):\n",
        "#     \"\"\"Loads and prepares the dataset from a text file.\"\"\"\n",
        "#     dataset = np.transpose(\n",
        "#         np.loadtxt('/content/gdrive/My Drive/iniMPSimEns_1000.txt'))\n",
        "#     num_examples = dataset.shape[0]\n",
        "#     original_train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "#         dataset[0:int(num_examples*(1-test_size))])\n",
        "#     val_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "#         dataset[int(num_examples*(1-test_size)):])\n",
        "\n",
        "#     augmented_train_dataset = original_train_dataset.map(augment_image).cache().shuffle(1024).batch(BATCH_SIZE)\n",
        "#     original_train_dataset = original_train_dataset.map(map_image).cache().shuffle(1024).batch(BATCH_SIZE)\n",
        "#     val_dataset = val_dataset.map(map_image).cache().batch(BATCH_SIZE)\n",
        "\n",
        "#     return augmented_train_dataset, original_train_dataset, val_dataset, num_examples\n",
        "\n",
        "def get_datasets(test_size):\n",
        "    \"\"\"Loads and prepares the dataset from a 2D array loaded from a text file.\"\"\"\n",
        "    dataset = np.transpose(\n",
        "        np.loadtxt('/content/gdrive/My Drive/iniMPSimEns_1000.txt'))\n",
        "    num_examples = dataset.shape[0]\n",
        "    original_train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        dataset[0:int(num_examples*(1-test_size))])\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        dataset[int(num_examples*(1-test_size)):])\n",
        "\n",
        "    original_train_dataset = original_train_dataset.map(map_image).batch(int(num_examples*(1-test_size)))\n",
        "    val_dataset = val_dataset.map(map_image).batch(int(num_examples*(1-test_size)))\n",
        "\n",
        "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        width_shift_range = 0.15,\n",
        "        horizontal_flip = True,\n",
        "        vertical_flip = True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "    val_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "\n",
        "    for input_images, images in original_train_dataset:\n",
        "        x_train, y_train = input_images, images\n",
        "    for input_images, images in val_dataset:\n",
        "        x_val, y_val = input_images, images\n",
        "\n",
        "    train_generator = train_datagen.flow(x_train, y_train, batch_size=BATCH_SIZE)\n",
        "    val_generator = val_datagen.flow(x_val, y_val, batch_size=BATCH_SIZE)\n",
        "\n",
        "    return train_generator, original_train_dataset, val_generator, num_examples\n",
        "\n",
        "def display_three_train_images(train_dataset):\n",
        "    \"\"\"Displays 3 images from the training dataset\"\"\"\n",
        "    plt.figure(figsize=(5, 14))\n",
        "    for input_images, _ in train_dataset.take(1):\n",
        "        for i in range(3):\n",
        "            plt.subplot(3, 1, i+1)\n",
        "            plt.imshow(np.squeeze(input_images[i]), cmap='gray')\n",
        "    plt.show()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZReHg3QTtfe9"
      },
      "source": [
        "class Sampling(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Generates a random sample and combines with the encoder output\n",
        "\n",
        "        Args:\n",
        "          inputs -- output tensor from the encoder\n",
        "\n",
        "        Returns:\n",
        "          `inputs` tensors combined with a random sample\n",
        "        \"\"\"\n",
        "\n",
        "        # unpack the output of the encoder\n",
        "        mu, sigma = inputs\n",
        "\n",
        "        # get the size and dimensions of the batch\n",
        "        batch = tf.shape(mu)[0]\n",
        "        dim = tf.shape(mu)[1]\n",
        "\n",
        "        # generate a random tensor\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "\n",
        "        # combine the inputs and noise\n",
        "        return mu + tf.exp(0.5 * sigma) * epsilon\n",
        "\n",
        "\n",
        "def encoder_layers(inputs, latent_dim):\n",
        "    \"\"\"Defines the encoder's layers.\n",
        "    Args:\n",
        "      inputs -- batch from the dataset\n",
        "      latent_dim -- dimensionality of the latent space\n",
        "\n",
        "    Returns:\n",
        "      mu -- learned mean\n",
        "      sigma -- learned standard deviation\n",
        "      batch_2.shape -- shape of the features before flattening\n",
        "    \"\"\"\n",
        "\n",
        "    # add the Conv2D layers followed by BatchNormalization\n",
        "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, \n",
        "                               padding=\"same\", activation='relu',\n",
        "                               name=\"encode_conv1\")(inputs)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, \n",
        "                               padding='same', activation='relu',\n",
        "                               name=\"encode_conv2\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=2, \n",
        "                               padding='same', activation='relu',\n",
        "                               name=\"encode_conv3\")(x)\n",
        "    # x = tf.keras.layers.BatchNormalization()(x)\n",
        "    # x = tf.keras.layers.Conv2D(filters=256, kernel_size=3, strides=2, \n",
        "    #                            padding='same', activation='relu',\n",
        "    #                            name=\"encode_conv4\")(x)    \n",
        "\n",
        "    # assign to a different variable so you can extract the shape later\n",
        "    batch_3 = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    # flatten the features and feed into the Dense network\n",
        "    x = tf.keras.layers.Flatten(name=\"encode_flatten\")(batch_3)\n",
        "\n",
        "    # we arbitrarily used ... units here but feel free to change\n",
        "    x = tf.keras.layers.Dense(1024, activation='relu', name=\"encode_dense\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    # add output Dense networks for mu and sigma, units equal to the declared latent_dim.\n",
        "    mu = tf.keras.layers.Dense(latent_dim, name='latent_mu')(x)\n",
        "    sigma = tf.keras.layers.Dense(latent_dim, name='latent_sigma')(x)\n",
        "\n",
        "    return mu, sigma, batch_3.shape\n",
        "\n",
        "def encoder_model(latent_dim, input_shape):\n",
        "    \"\"\"Defines the encoder model with the Sampling layer\n",
        "    Args:\n",
        "      latent_dim -- dimensionality of the latent space\n",
        "      input_shape -- shape of the dataset batch\n",
        "\n",
        "    Returns:\n",
        "      model -- the encoder model\n",
        "      conv_shape -- shape of the features before flattening\n",
        "    \"\"\"\n",
        "\n",
        "    # declare the inputs tensor with the given shape\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "    # get the output of the encoder_layers() function\n",
        "    mu, sigma, conv_shape = encoder_layers(inputs, latent_dim=LATENT_DIM)\n",
        "\n",
        "    # feed mu and sigma to the Sampling layer\n",
        "    z = Sampling()((mu, sigma))\n",
        "\n",
        "    # build the whole encoder model\n",
        "    model = tf.keras.Model(inputs, outputs=[mu, sigma, z])\n",
        "\n",
        "    return model, conv_shape\n",
        "\n",
        "\n",
        "# Define decision variables for adding Cropping2D layers in decoder layers\n",
        "\n",
        "# topcrop_after_upsampling0 = (math.ceil(math.ceil(math.ceil(IMAGE_HEIGHT/2)/2)/2) % 2 != 0)\n",
        "# leftcrop_after_upsampling0 = (math.ceil(math.ceil(math.ceil(IMAGE_WIDTH/2)/2)/2) % 2 != 0)\n",
        "topcrop_after_upsampling1 = (math.ceil(math.ceil(IMAGE_HEIGHT/2)/2) % 2 != 0)\n",
        "leftcrop_after_upsampling1 = (math.ceil(math.ceil(IMAGE_WIDTH/2)/2) % 2 != 0)\n",
        "topcrop_after_upsampling2 = (math.ceil(IMAGE_HEIGHT/2) % 2 != 0)\n",
        "leftcrop_after_upsampling2 = (math.ceil(IMAGE_WIDTH/2) % 2 != 0)\n",
        "topcrop_after_upsampling3 = (IMAGE_HEIGHT % 2 != 0)\n",
        "leftcrop_after_upsampling3 = (IMAGE_WIDTH % 2 != 0)\n",
        "\n",
        "\n",
        "def decoder_layers(inputs, conv_shape, topcrop_after_upsampling1, \n",
        "                   leftcrop_after_upsampling1, topcrop_after_upsampling2, \n",
        "                   leftcrop_after_upsampling2,\n",
        "                   topcrop_after_upsampling3, leftcrop_after_upsampling3):\n",
        "    \"\"\"Defines the decoder layers.\n",
        "    Args:\n",
        "      inputs -- output of the encoder\n",
        "      conv_shape -- shape of the features before flattening\n",
        "\n",
        "    Returns:\n",
        "      tensor containing the decoded output\n",
        "    \"\"\"\n",
        "\n",
        "    # feed to a Dense network with units computed from the conv_shape dimensions\n",
        "    units = conv_shape[1] * conv_shape[2] * conv_shape[3]\n",
        "    x = tf.keras.layers.Dense(units, activation='relu', name=\"decode_dense1\")(inputs)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    # reshape output using the conv_shape dimensions\n",
        "    x = tf.keras.layers.Reshape((conv_shape[1], conv_shape[2], conv_shape[3]), \n",
        "                                name=\"decode_reshape\")(x)\n",
        "\n",
        "    # upsample the features back to the original dimensions\n",
        "    # for that, make sure to add Cropping2D layers after upsampling when needed\n",
        "    # x = tf.keras.layers.Conv2DTranspose(filters=256, kernel_size=3, strides=2, \n",
        "    #                                     padding='same', activation='relu',\n",
        "    #                                     name=\"decode_conv2d_0\")(x)\n",
        "    # x = tf.keras.layers.BatchNormalization()(x)\n",
        "    # if topcrop_after_upsampling0:\n",
        "    #     x = tf.keras.layers.Cropping2D(cropping=((1, 0), (0, 0)))(x)\n",
        "    # if leftcrop_after_upsampling0:\n",
        "    #     x = tf.keras.layers.Cropping2D(cropping=((0, 0), (1, 0)))(x)    \n",
        "    x = tf.keras.layers.Conv2DTranspose(filters=128, kernel_size=3, strides=2, \n",
        "                                        padding='same', activation='relu',\n",
        "                                        name=\"decode_conv2d_1\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    if topcrop_after_upsampling1:\n",
        "        x = tf.keras.layers.Cropping2D(cropping=((1, 0), (0, 0)))(x)\n",
        "    if leftcrop_after_upsampling1:\n",
        "        x = tf.keras.layers.Cropping2D(cropping=((0, 0), (1, 0)))(x)\n",
        "    x = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=2, \n",
        "                                        padding='same', activation='relu',\n",
        "                                        name=\"decode_conv2d_2\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    if topcrop_after_upsampling2:\n",
        "        x = tf.keras.layers.Cropping2D(cropping=((1, 0), (0, 0)))(x)\n",
        "    if leftcrop_after_upsampling2:\n",
        "        x = tf.keras.layers.Cropping2D(cropping=((0, 0), (1, 0)))(x)\n",
        "    x = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=2, \n",
        "                                        padding='same', activation='relu',\n",
        "                                        name=\"decode_conv2d_3\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    if topcrop_after_upsampling3:\n",
        "        x = tf.keras.layers.Cropping2D(cropping=((1, 0), (0, 0)))(x)\n",
        "    if leftcrop_after_upsampling3:\n",
        "        x = tf.keras.layers.Cropping2D(cropping=((0, 0), (1, 0)))(x)\n",
        "    x = tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=1, \n",
        "                                        padding='same', activation='sigmoid',\n",
        "                                        name=\"decode_final\")(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def decoder_model(latent_dim, conv_shape):\n",
        "    \"\"\"Defines the decoder model.\n",
        "    Args:\n",
        "      latent_dim -- dimensionality of the latent space\n",
        "      conv_shape -- shape of the features before flattening\n",
        "\n",
        "    Returns:\n",
        "      model -- the decoder model\n",
        "    \"\"\"\n",
        "\n",
        "    # set the inputs to the shape of the latent space\n",
        "    inputs = tf.keras.layers.Input(shape=(latent_dim,))\n",
        "\n",
        "    # get the output of the decoder layers\n",
        "    outputs = decoder_layers(inputs, conv_shape, topcrop_after_upsampling1, \n",
        "                             leftcrop_after_upsampling1, \n",
        "                             topcrop_after_upsampling2, \n",
        "                             leftcrop_after_upsampling2, \n",
        "                             topcrop_after_upsampling3, \n",
        "                             leftcrop_after_upsampling3)\n",
        "\n",
        "    # declare the inputs and outputs of the model\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "def kl_reconstruction_loss(inputs, outputs, mu, sigma):\n",
        "    \"\"\" Computes the Kullback-Leibler Divergence (KLD)\n",
        "    Args:\n",
        "      inputs -- batch from the dataset\n",
        "      outputs -- output of the Sampling layer\n",
        "      mu -- mean\n",
        "      sigma -- standard deviation\n",
        "\n",
        "    Returns:\n",
        "      KLD loss\n",
        "    \"\"\"\n",
        "    kl_loss = 1 + sigma - tf.square(mu) - tf.math.exp(sigma)\n",
        "    kl_loss = tf.reduce_mean(kl_loss) * -0.5\n",
        "\n",
        "    return kl_loss\n",
        "\n",
        "def vae_model(encoder, decoder, input_shape):\n",
        "    \"\"\"Defines the VAE model\n",
        "    Args:\n",
        "      encoder -- the encoder model\n",
        "      decoder -- the decoder model\n",
        "      input_shape -- shape of the dataset batch\n",
        "\n",
        "    Returns:\n",
        "      the complete VAE model\n",
        "    \"\"\"\n",
        "    # set the inputs\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "    # get mu, sigma, and z from the encoder output\n",
        "    mu, sigma, z = encoder(inputs)\n",
        "\n",
        "    # get reconstructed output from the decoder\n",
        "    reconstructed = decoder(z)\n",
        "\n",
        "    # define the inputs and outputs of the VAE\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=reconstructed)\n",
        "\n",
        "    # add the KL loss\n",
        "    loss = kl_reconstruction_loss(inputs, z, mu, sigma)\n",
        "    model.add_loss(loss)\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_models(input_shape, latent_dim):\n",
        "    \"\"\"Returns the encoder, decoder, and vae models\"\"\"\n",
        "    encoder, conv_shape = encoder_model(latent_dim=latent_dim, input_shape=input_shape)\n",
        "    decoder = decoder_model(latent_dim=latent_dim, conv_shape=conv_shape)\n",
        "    vae = vae_model(encoder, decoder, input_shape=input_shape)\n",
        "    return encoder, decoder, vae"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kuHjYojtmHz"
      },
      "source": [
        "# Define a VAE class via model subclassing\n",
        "loss_metrics = tf.keras.metrics.Mean()\n",
        "val_loss_metrics = tf.keras.metrics.Mean()\n",
        "\n",
        "class VAE(tf.keras.Model):\n",
        "    def __init__(self, encoder, decoder, variational_autoencoder):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.vae = variational_autoencoder\n",
        "\n",
        "    # override train_step method\n",
        "    def train_step(self, images):\n",
        "        if isinstance(images, tuple):\n",
        "            images = images[0]\n",
        "        with tf.GradientTape() as tape:\n",
        "            # feed a batch to the VAE model\n",
        "            reconstructed = self.vae(images)\n",
        "            # compute reconstruction loss\n",
        "            flattened_inputs = tf.reshape(images, [-1])\n",
        "            flattened_outputs = tf.reshape(reconstructed, [-1])\n",
        "            loss = self.compiled_loss(flattened_inputs, flattened_outputs) \\\n",
        "                   * images.shape[1] * images.shape[2]\n",
        "            # add KLD regularization loss\n",
        "            loss += sum(self.vae.losses)\n",
        "\n",
        "        # compute the gradients and update the model weights\n",
        "        grads = tape.gradient(loss, self.vae.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.vae.trainable_weights))\n",
        "\n",
        "        # update metrics\n",
        "        loss_metrics.update_state(loss)\n",
        "        \n",
        "        # return a dict mapping metrics names to current value\n",
        "        return {'loss': loss_metrics.result()}\n",
        "\n",
        "    # override test_step method\n",
        "    def test_step(self, images):\n",
        "        if isinstance(images, tuple):\n",
        "            images = images[0]\n",
        "        # compute predictions\n",
        "        reconstructed = self.vae(images)\n",
        "        # compute loss\n",
        "        flattened_inputs = tf.reshape(images, [-1])\n",
        "        flattened_outputs = tf.reshape(reconstructed, [-1])\n",
        "        loss = self.compiled_loss(flattened_inputs, flattened_outputs) \\\n",
        "               * images.shape[1] * images.shape[2]\n",
        "        # add KLD regularization loss\n",
        "        loss += sum(self.vae.losses)\n",
        "        # update metrics\n",
        "        val_loss_metrics.update_state(loss)\n",
        "        # return a dict mapping metrics names to current value\n",
        "        return {'loss': val_loss_metrics.result()}\n",
        "\n",
        "    def call(self, images):\n",
        "        if isinstance(images, tuple):\n",
        "            images = images[0]\n",
        "        return self.vae(images)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqBdfidbt3aF"
      },
      "source": [
        "def generate_and_save_images(model, epoch, step, test_input, vae_model):\n",
        "    \"\"\"Helper function to plot our 8 images\n",
        "\n",
        "    Args:\n",
        "\n",
        "    model -- the decoder model\n",
        "    epoch -- current epoch number during training\n",
        "    step -- current step number during training\n",
        "    test_input -- random tensor with shape (8, LATENT_DIM)\n",
        "    \"\"\"\n",
        "\n",
        "    # generate images from the test input\n",
        "    predictions = model.predict(test_input)\n",
        "\n",
        "    # plot the results\n",
        "    fig = plt.figure(figsize=(12, 14))\n",
        "\n",
        "    for i in range(predictions.shape[0]):\n",
        "        plt.subplot(8, 1, i + 1)\n",
        "        plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
        "        plt.axis('off')\n",
        "\n",
        "    # tight_layout minimizes the overlap between 2 sub-plots\n",
        "    fig.suptitle(\"epoch: {}, step: {}\".format(epoch, step))\n",
        "    plt.savefig('image_at_epoch_{:04d}_step{:04d}.png'.format(epoch, step))\n",
        "\n",
        "    if epoch != 0:\n",
        "        fig = plt.figure()\n",
        "        plt.scatter(range(len(vae_model.history.history['loss'])), vae_model.history.history['loss'])\n",
        "        plt.scatter(range(len(vae_model.history.history['val_loss'])), vae_model.history.history['val_loss'], color='red')\n",
        "        plt.savefig('Losses.png')\n",
        "    plt.show()\n",
        "\n",
        "def show_original_reconstructed_images(model, train_dataset):\n",
        "    plt.figure(figsize=(10, 14))\n",
        "    for input_images, _ in train_dataset.take(1):\n",
        "        reconstructed = model(input_images)\n",
        "        k = 0\n",
        "        for i in range(5):\n",
        "            #reconstructed_categorized = np.where(reconstructed >= 0.5, 1, 0)\n",
        "            plt.subplot(5, 2, k+1)\n",
        "            plt.imshow(np.squeeze(input_images[i]), cmap='gray')\n",
        "            plt.subplot(5, 2, k+2)\n",
        "            #plt.imshow(np.squeeze(reconstructed_categorized[i]), cmap='gray')\n",
        "            plt.imshow(np.squeeze(reconstructed[i]), cmap='gray')\n",
        "            k += 2\n",
        "        plt.savefig(\"reconstructed_images.png\")\n",
        "    plt.show()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaZthzIQcdMP"
      },
      "source": [
        "Setting a learning rate scheduler for selecting the learning rate parameter during training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwJjP5p7t7Ob"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load and prepare image dataset for training\n",
        "augmented_train_dataset, train_dataset, val_dataset, num_examples = get_datasets(test_size=TEST_SIZE)\n",
        "print(f\"Num of original examples: {num_examples}\")\n",
        "#display_three_train_images(train_dataset)\n",
        "\n",
        "# Create a callback that saves the model's weights every few epochs during training\n",
        "checkpoint_path = 'checkpoint/cp-cp{epoch:04d}.ckpt'\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_path,\n",
        "    verbose = 1,\n",
        "    save_weights_only = True,\n",
        "    save_freq = math.ceil(num_examples/BATCH_SIZE) * 100\n",
        ")\n",
        "\n",
        "# Create custom callback to display outputs (via helper function) at the end of each epoch of training\n",
        "class CustomCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        # Generate random vector as test input to the decoder\n",
        "        random_vector_for_generation = tf.random.normal(shape=[8, LATENT_DIM])\n",
        "        # Generate and save images\n",
        "        display.clear_output(wait=False)\n",
        "        if epoch % 100 == 0:\n",
        "            generate_and_save_images(decoder, epoch, \n",
        "                                     math.ceil(num_examples/BATCH_SIZE), \n",
        "                                     random_vector_for_generation, vae)\n",
        "        print('End of epoch {} - mean loss = {}'.format(epoch, logs[keys[0]]))\n",
        "\n",
        "# Create callback for adjusting learning rate during training\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-6 * 10**(epoch / 20))\n",
        "\n",
        "# Get the encoder, decoder and 'master' model (called vae)\n",
        "encoder, decoder, var_autoencoder = get_models(\n",
        "    input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS,), \n",
        "    latent_dim=LATENT_DIM)\n",
        "\n",
        "# Instantiate VAE class\n",
        "vae = VAE(encoder, decoder, var_autoencoder)\n",
        "\n",
        "# Compile model\n",
        "vae.compile(\n",
        "    optimizer = tf.keras.optimizers.Adam(lr=1e-6),\n",
        "    loss = tf.keras.losses.BinaryCrossentropy()\n",
        ")\n",
        "\n",
        "# Generate random vector as test input to the decoder\n",
        "random_vector_for_generation = tf.random.normal(shape=[8, LATENT_DIM])\n",
        "\n",
        "# Initialize the helper function to display outputs from an untrained model\n",
        "generate_and_save_images(decoder, 0, 0, random_vector_for_generation, vae)\n",
        "\n",
        "# Training loop using original dataset\n",
        "history = vae.fit(augmented_train_dataset, epochs=100, verbose=1, \n",
        "                  callbacks=[cp_callback, CustomCallback(), lr_schedule])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "6BSZVxR26H6D",
        "outputId": "66b9e7d8-c4f6-4580-f54a-8f36a05895a9"
      },
      "source": [
        "# Plot losses against learning rates\n",
        "plt.semilogx(vae.history.history['lr'], vae.history.history['loss'])\n",
        "plt.axis([1e-6, 0.01, 14500, 17400])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1e-06, 0.01, 14500.0, 17400.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAD8CAYAAABZ/vJZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXyU9bn38c+VyUqAQBa2EAnI4gISS9hcEFDrUlvUuqDnuJy616U9ak/b0+c87dOn56m1LqdaN1TqikjVKq1bLYogIhJAFBQh7EEgYQnIlvV6/pgbjRRIMkwyM+T7fr3mlZlr7vuea36vkC/3bu6OiIhIcyXFugEREUlMChAREYmIAkRERCKiABERkYgoQEREJCIKEBERiUhyrBuIVG5urhcWFsa6DRGRhDJv3rxN7p4XjWUlbIAUFhZSUlIS6zZERBKKma2O1rK0CUtERCKiABERkYgk7Cas0vIdnPfgLEJmJCUZyUlGKMlIsq9/JicZoZARsq/fTw4FP5OSvnodfi+JlGD6lKSkcD2URGooPG1Kcvh5Sijpq0dqspEaCpGanERKyEhNTiItOURaShKpoSTSkpMws1gPlYhIi0jYAAklGZmpydTVO3XuVNfWU+dOffC6rh7q653a+nrq6p3aeg9eO3X1Tk3d1/W9P1tCanIS6clJpKeEyEgNkZESCj8PXrdLDZGZmkxmWjKZaSHapyXTMSOFjukpdMxIJisjhezMVHLbp5GeEmqRHkVEIpGwAdI7N5Nnrh4eteW5fzNcauucmvrgZ109NXXhkKqtr6e6tp7quvDPvfWaunqqauuorq2nquGjpo49NXXsqalnT20du6vr2B3Uyr+sYVdVHTura9kZ/DzYtS0zU0PktE8jv1MGvXLaUZDdjl457TguvxMF2Rla2xGRVpWwARJtZkZKyEgJEbP/6bs7u6rr2L6nhu27a9m+p4bKXTVs2VnF5p3VbN5RTcWXVazduot/fLaRTTuqv5o3v1MGw/tkM7JPDqMHdCGvQ1pMvoOItB0KkDhiZsGmrGS6ZzU+/c6qWlZt3sm81Vv5YMVmpn9ewUvz15GcZJw5sBuXjejFsN7ZWjMRkRZhiXo/kOLiYtd5IN9UX+8s2fAlL80vY0rJWrbvqWVA1w784KRCLhhSQChJQSLS1pnZPHcvjsqyFCCHp93Vdfx14Rc8OXsVi7/YzuCeWfzm3EEM6tmEVRsROWxFM0B0HshhKiM1xEVDC/jbzSfxh/FFrKvcw7gH3uOXryxi+56aWLcnIocBBchhzswYV5TPtNtO4V9H9OKpD1Zz2t3vMnfVlli3JiIJTgHSRmRlpPDrcQN55cYTyUxL5pIJH/Dk+6tI1E2YIhJ7CpA25rienXj5xhMZPSCPX05dzG1TFrK7ui7WbYlIAlKAtEFZGSlMuKyYW0/vz18+Wsf3H3qfLyp3x7otEUkwCpA2KinJuOXUfky8Yihrt+zigofep7T8y1i3JSIJRAHSxo05qguTrxtBdZ1zwcOzWbBma6xbEpEEoQARju2RxYs3jCQrI4VLH53Du0srYt2SiCQABYgA0Csnkz9fP5LeuZlc9cRcXv14faxbEpE4pwCRr3TpkM7k60Zw/BGd+NHkBfx98YZYtyQicUwBIt/QMT2FiVcOZWB+FjdOms87n5fHuiURiVMKEPknHdJTePIHwxjQrQPXPT2PWaWbYt2SiMQhBYjsV1ZGCk//YDh9cjO56sm5zFmxOdYtiUicUYDIAXXOTOWZq4eT3ymDHzwxl4/WVsa6JRGJIwoQOajc9mk8e/UIctqncfnjc/j0i+2xbklE4kSjAWJmE82s3MwWNag9b2YfBY9VZvZRg/d+bmalZva5mZ3RoH5mUCs1s581qPc2szlB/XkzS43mF5RD1y0rnWevHk5mWjKXPT6H0vIdsW5JROJAU9ZAngDObFhw94vdvcjdi4AXgZcAzOwYYDxwbDDPg2YWMrMQ8ABwFnAMcEkwLcDvgHvdvS+wFbjqkL+VRF1BdjuevXo4Zsa/PPYBazbvinVLIhJjjQaIu88A9nvzCAvfbPsi4LmgNA6Y7O5V7r4SKAWGBY9Sd1/h7tXAZGBcMP9Y4IVg/ieBcw/h+0gL6pPXnmeuHsaemnouefQD1m5RiIi0ZYe6D+RkYKO7Lwte5wNrG7xfFtQOVM8BKt29dp+6xKmjunXkmauGs6OqlvETtCYi0pYdaoBcwtdrHy3OzK41sxIzK6mo0PWaYmVQzyyevXo4O6trGT9hNqs374x1SyISAxEHiJklA+cDzzcorwMKGrzuGdQOVN8MdAqW1bC+X+4+wd2L3b04Ly8v0tYlCgbmh0Nkd00d4yd8wKpNChGRtuZQ1kBOA5a4e1mD2lRgvJmlmVlvoB/wITAX6BcccZVKeEf7VA/fT/Ud4IJg/iuAVw6hJ2lFx/bIYtI1I6iqrefiCbNZtlH3ExFpS5pyGO9zwGxggJmVmdneo6TGs8/mK3dfDEwBPgXeAG5097pgH8dNwJvAZ8CUYFqAnwK3mlkp4X0ijx/615LWcnT3jjx3zQjqHS58ZDYLdbKhSJth4ZWAxFNcXOwlJSWxbkMCqzfv5F8fn8OWHdU8enkxJ/TNjXVLIrIfZjbP3YujsSydiS5R0SsnkxeuP4Gendtx5Z/m8qYuBS9y2FOASNR07ZjO89eN4JgeHbnhmXlMmrMm1i2JSAtSgEhUdWqXyrNXD2dU/zz+8y+f8Ps3l5Com0lF5OAUIBJ1mWnJPHZ5MeOHFvDAO8u5dcpCqmvrY92WiERZcuOTiDRfciiJ354/iJ6dM7jr70vZsG0PD182hKyMlFi3JiJRojUQaTFmxk1j+3HPRYMpWb2F8x6cxUqdcChy2FCASIs7/1s9eeaq4VTuqmHcH99j5jJdhkbkcKAAkVYxvE8Or9x4It2zMrjyT3N5YtZK7VwXSXAKEGk1BdntePGHJzBmQBd+9ddP+emLH7Onpi7WbYlIhBQg0qrapyUz4bIh3Dy2L1NKyjj/wfd1SXiRBKUAkVaXlGTc9u0BTLyymLKtuzjn/pn849ONsW5LRJpJASIxM/aorrx6y8kckdOOq58q4Y7Xl1BTp/NFRBKFAkRiqiC7HS9cfwKXDCvg4XeXc/Ejs3WrXJEEoQCRmEtPCfHb84/jvkuOZ+nGHXznvpm8/sn6WLclIo1QgEjc+N7gHrx2y8n0zs3khmfn84u/fMLuah2lJRKvFCASV47Iacefrz+B60b14dk5a/jO/TP5uEw3qRKJRwoQiTupyUn8/OyjmXT1cHZX13H+g+9z/7Rl1GoHu0hcUYBI3Dqhby5v/GgUZw3qzt1vLeWiR2azomJHrNsSkYACROJaVrsU7r/keP4wvojS8h2c9YeZPDZzBXX1ugyKSKwpQCQhjCvK561bT+Gkvrn85tXPuOiR2SzX2ohITClAJGF07ZjOY1cUc+/Fgykt38HZf5jJQ9OX6+RDkRhRgEhCMTPOO74nb/37KEYPyON3byzhu/e/x0drdaSWSGtTgEhC6tIxnUcuK+aRy4ZQuauG8x6cxa+mLmZHVW2sWxNpMxQgktDOOLYbb906istH9OLJ2as49e7p/O3jL3SvEZFWoACRhNchPYX/M24gf/nhieR1SOOmSQu47PEPtZNdpIUpQOSwUVTQiVduPIlfjzuWhWWVnPk/M7jzjSXarCXSQhQgclgJJRmXjyzk7dtG893BPXhw+nLG3DWdP5espV7njohElQJEDkt5HdK456IiXr7xRAo6Z/CTFz5m3AOzmLtqS6xbEzlsKEDksFZU0IkXbziBP4wvYtOOKi58eDbXPz2PlZt2xro1kYTXaICY2UQzKzezRfvUbzazJWa22MzuDGqFZrbbzD4KHg83mH6ImX1iZqVmdp+ZWVDPNrO3zGxZ8LNztL+ktG1mxriifN6+bTS3nd6fGcsqOP2ed/nV1MVs2Vkd6/ZEElZT1kCeAM5sWDCzMcA4YLC7Hwvc1eDt5e5eFDyub1B/CLgG6Bc89i7zZ8A0d+8HTAtei0RdRmqIm0/tx/SfjOaioQU8NXsVp9z5Dn98exm7qrWjXaS5Gg0Qd58B7Lvh+AbgDnevCqYpP9gyzKw70NHdP/DwAfpPAecGb48DngyeP9mgLtIiunRI5/+dN4g3fjyK4X1yuOvvSxl153Senr1Kl0URaYZI94H0B042szlm9q6ZDW3wXm8zWxDUTw5q+UBZg2nKghpAV3ffe//SDUDXA32omV1rZiVmVlJRURFh6yJh/bt24LErinnh+pH0zm3Hf72ymNPueVcnIoo0UaQBkgxkAyOAnwBTgn0a64Ej3P144FZgkpl1bOpCg7WTA/7LdfcJ7l7s7sV5eXkRti7yTcWF2Uy5biR/unIoGSkhbpq0gPMefF9HbIk0ItIAKQNe8rAPgXog192r3H0zgLvPA5YTXltZB/RsMH/PoAawMdjEtXdT10E3h4m0BDNjzFFdePWWk7nz+8exfttuLnx4Ntc+VaKbWIkcQKQB8jIwBsDM+gOpwCYzyzOzUFDvQ3hn+YpgE9V2MxsRrKlcDrwSLGsqcEXw/IoGdZFWF0oyLhpawDu3h4/YmlW6idPvncF/vbyIii+rYt2eSFyxxrb1mtlzwGggF9gI/BJ4GpgIFAHVwO3u/raZfR/4NVBDeK3kl+7+12A5xYSP6MoAXgdudnc3sxxgCnAEsBq4yN0b3XZQXFzsJSUlzf2+Is1S8WUV901bxqQP15CenMR1pxzJ1Sf3pl1qcqxbE4mImc1z9+KoLCtRdxYqQKQ1La/Ywe/f+Jw3Fm8gr0MaPzq1HxcPLSAlpHNxJbFEM0D02y/SBEfmtefhy4bw4g0jKcxpx/96eRHfvneGjtiSNk0BItIMQ3qFj9h67PJiUkLGTZMWMO6BWcwq3RTr1kRanQJEpJnMjNOO6crrPxrFXRcOZvOOav7lsTlc9vgcFq3bFuv2RFqN9oGIHKI9NXU888Fq/vhOKZW7ajjnuO7c/u0BFOZmxro1kX+inegoQCT+bN9Tw6MzVvDYzJXU1NUzflgBt4ztR5eO6bFuTeQrChAUIBK/yr/cw/3TSnnuwzWkhJL4wUmFXDvqSLIyUmLdmogCBBQgEv9Wb97J3X9fytSFX5CVkcL1pxzJlScUkpEainVr0oYpQFCASOJY/MU27nrzc975vIIuHdK4+dR+jNc5JBIjOg9EJIEc2yOLP/3bMP58/UgKczL5r5cXcfo97/LaJ+t1DokkNAWISCsZWpjN89eN4E9XDiUtOcQPn53PeQ++z4crddVfSUwKEJFWtPeqv6/96GTuvOA4Nmzbw0WPzOa6p0tYs3lXrNsTaRYFiEgMhJKMi4rDV/29/dv9mblsE6fd+y6/f3MJO6t0e11JDAoQkRjKSA1x09h+vH3baL4zqDsPvLOcsXdP5y8Lyqiv1/4RiW8KEJE40C0rnXsvLuLFG0bStWM6//78Qs5/6H3mr9ka69ZEDkgBIhJHhvTK5uUfnshdFw7mi8rdnP/g+/x48gLWb9sd69ZE/okCRCTOJCUZFwzpyTu3j+amMX15bdEGxtw1nXveWsquau0fkfihABGJU5lpydx+xgCm3XoKpx3dlfumLWP076fz55K12j8icUEBIhLnCrLb8cdLv8WLN5xAj04Z/OSFj/nuH9/j/eW6B4nElgJEJEEM6dWZl244gT+ML6JyVw2XPjqHa54qYUXFjli3Jm2UAkQkgSQlGeOK8pl22yn85IwBzF6+mW/fO4NfTV3Mlp3VsW5P2hgFiEgCSk8JceOYvkz/yWguHlrAU7NXccrv3+Hhd5ezp6Yu1u1JG6EAEUlgue3T+O/zBvHmj0cxrDCbO15fwql3v8vLC9ZpR7u0OAWIyGGgX9cOPH7lUCZdM5zOmSn8+PmPOOf+95j+ebmu+CstRgEichg54chcpt54Ev9zcRFfVtVw5Z/mMn7CBzqjXVqEbiglcpiqrq3nuQ/XcP/by9i0o5rTju7Kv5/ej2N7ZMW6NYkh3ZEQBYhIU+2sqmXieyt5dOYKtu+p5ayB3fjxaf0Z0K1DrFuTGFCAoAARaa5tu2t4/L2VTHxvJTurazl7UHduGdtPQdLGKEBQgIhEqnJXNRNmrODJ91exs7qOM4/txk1j+zIwX5u22oJWvSe6mU00s3IzW7RP/WYzW2Jmi83szgb1n5tZqZl9bmZnNKifGdRKzexnDeq9zWxOUH/ezFKj8cVEZP86tUvlP848ilk/G8stY/sya/kmzrn/Pa56Yi4LtLNdmqHRNRAzGwXsAJ5y94FBbQzwC+A77l5lZl3cvdzMjgGeA4YBPYB/AP2DRS0FTgfKgLnAJe7+qZlNAV5y98lm9jCw0N0faqxxrYGIRMe23TU89f4qHp+1kspdNZzYN4cbR/dl5JE5mFms25Moa9U1EHefAWzZp3wDcIe7VwXTlAf1ccBkd69y95VAKeEwGQaUuvsKd68GJgPjLPzbORZ4IZj/SeDcQ/xOItIMWRkp3HxqP2b9dCy/OPtolm7cwaWPzeH8h97njUUbqNMJiXIAkZ4H0h84Odj09K6ZDQ3q+cDaBtOVBbUD1XOASnev3acuIq0sMy2Za0b1YeZ/jOH/njuQii+ruP6ZeZx2z7s8O2e1LpEi/yTSAEkGsoERwE+AKdYK67pmdq2ZlZhZSUVFRUt/nEiblJ4S4rIRvZh++2j+eOnxdEhP5hd/WcSJd7zNvW8tZdOOqli3KHEiOcL5ygjvt3DgQzOrB3KBdUBBg+l6BjUOUN8MdDKz5GAtpOH0/8TdJwATILwPJMLeRaQJkkNJnHNcD74zqDsfrNjCozNX8Idpy3jo3eWcV5TPVSf3pn9XHQLclkUaIC8DY4B3zKw/kApsAqYCk8zsHsI70fsBHwIG9DOz3oQDYjxwqbu7mb0DXEB4v8gVwCuH8H1EJMrMjJFH5jDyyBxKy3fwp1kreXF+Gc+XrOWkvrn824mFjBnQhaQk7XBva5pyFNZzwGjCaxgbgV8CTwMTgSKgGrjd3d8Opv8F8AOgFvixu78e1M8G/gcIARPd/b+Deh/C4ZENLAD+de/O+YPRUVgisbN1ZzWTPlzDU7NXsXF7FYU57bjihEIuLC6gfVqk/y+V1qATCVGAiMSDmrp6Xl+0gSdmrWT+mkrapyXz/W/lc9nIQvp2aR/r9mQ/FCAoQETizcK1lTw5exV/W7ie6rp6Tuqby2Uje3HqUV1IDunC3/FCAYICRCRebdpRxfNz1/LMB6tZv20P3TqmM35YAeOHHkG3rPRYt9fmKUBQgIjEu9q6eqYtKefZOWuYsbSCUJJx2tFduHR4L07um6ud7jESzQDR3i4RaRHJoSTOOLYbZxzbjdWbdzLpwzX8uaSMNxdvpGfnDMYPLeDC4gK6dtRaSaLSGoiItJqq2jre+nQjk+as4f3lmwklGaP753FhcU/GHtWV1GTtK2lpWgMRkYSUlhzinON6cM5xPVi1aSeT567lpfllTFtSTnZmKuOKenDBkJ66a2KC0BqIiMRUbV09M5dt4oV5Zbz16Uaq6+o5qlsHLhjSk3FF+eR1SIt1i4cV7URHASJyOKrcVc1fF37BC/PKWFi2jVCScUr/PM47Pp/Tj+lKekoo1i0mPAUIChCRw92yjV/y4vx1vLxgHRu276FDWjJnD+rOucfnM7x3to7iipACBAWISFtRV+98sGIzL84v441FG9hVXUf3rHS+N7gH44ryObp7B934qhkUIChARNqiXdW1vPXpRl756AtmLK2gtt7p16U93xvcg+8O7kFhbmasW4x7ChAUICJt3Zad1bz6yXqmfrSOuavC93IflJ/F9wb34OzjupPfKSPGHcYnBQgKEBH52heVu3n14/VMXfgFn6zbBsC3jujEd4L7megSKl9TgKAAEZH9W7VpJ69+sp6/fbyez9ZvB6C4V2fOHtSdswZ1o3tW214zUYCgABGRxi2v2MFrH6/n1U/Ws2TDlwAM6dWZswaGL7FSkN0uxh22PgUIChARaZ7lFTt4/ZP1vPrJhq/WTAbmd+SMY7pxxsBu9OvSvk0czaUAQQEiIpFbvXknby7ewBuLNjB/TSUAvXLacdrRXTnt6K4MLex82N7DRAGCAkREomPDtj1MW7KRf3y6kVnLN1NdW09WRgqj+udx6lFdOKV/Hp0zU2PdZtQoQFCAiEj07ayqZeayTfzjs41M/7ycTTuqSTL41hGdGXt0F049qiv9uyb2pi4FCAoQEWlZ9fXOJ+u2MW1JOW8v2ciideH9JvmdMhh7VBdO6pfLiD45ZGWkxLjT5lGAoAARkda1cfse3llSzrQl5by3bBO7a+pIMhiYn8XII3M4pntHenZuxxHZ7chtnxq3aykKEBQgIhI7VbV1LFy7jVmlm5i9fDML1m6lpu7rv6UZKSH6d21PcWE2Qws7M6RXdtxcll4BggJEROLHnpo61m7ZxZotu1i7ZRert+xi8brtfFRWSXVtPQC9czMZeWQOI/vkMPLIHHLbxyZQdEdCEZE4kp4Sol/XDvTr2uEb9araOhat207Jqi3MWbmFqR99waQ5awDo37U9w3vnMKx3NsN6ZyfkveG1BiIi0kpq6+r5ZN02Zq/YzOzlm5m/eis7q+sAKMxpF4RJDsN7Z9Ozc0aL7EfRJiwUICKS+Grr6vl0/XY+XBleQ5m7aguVu2oA6JGV/nWg9MmmT25mVAJFAYICREQOP/X1ztLyL8OBsiIcKpt2VAGQ2z6V4l7ZDOnVmW/16szA/I6kJTf/Fr8KEBQgInL4c3dWbtr51RrKvNVbWbNlFwCpoSQG5nfkW0eEA+X4Izo16UrDChAUICLSNpV/uYf5qyuZt3oLC9ZU8vG6bV8d6dU9K53BPTtRdEQnBvfsxKCeWbRP++axUq16FJaZTQTOAcrdfWBQ+xVwDVARTPaf7v6amRUCnwGfB/UP3P36YJ4hwBNABvAa8CN3dzPLBp4HCoFVwEXuvvXQv5qIyOGnS4d0zhzYjTMHdgOguraez9ZvZ/6arSxYU8nCskreWLwBgCSDfl06MLggi8EF4VCJpkbXQMxsFLADeGqfANnh7nftM20h8Le90+3z3ofALcAcwgFyn7u/bmZ3Alvc/Q4z+xnQ2d1/2ljjWgMREdm/LTurWVhWyUdBoCxcW8nWYOf86t+d03prIO4+IwiGiJlZd6Cju38QvH4KOBd4HRgHjA4mfRKYDjQaICIisn/ZmamMGdCFMQO6AOF9KWu37GbB2q2c+7vofc6hXPD+JjP72MwmmlnnBvXeZrbAzN41s5ODWj5Q1mCasqAG0NXd1wfPNwBdD/SBZnatmZWYWUlFRcWBJhMRkQbMjCNy2jGuKL/xiZsh0gB5CDgSKALWA3cH9fXAEe5+PHArMMnMOjZ1oR7ennbAbWruPsHdi929OC8vL8LWRUQkGiIKEHff6O517l4PPAoMC+pV7r45eD4PWA70B9YBPRssomdQA9gYbOLau6mrPJKeRESkdUUUIHv/4AfOAxYF9TwzCwXP+wD9gBXBJqrtZjbCwqdSXg68Esw/FbgieH5Fg7qIiMSxphzG+xzhndy5ZlYG/BIYbWZFhDc3rQKuCyYfBfzazGqAeuB6d98SvPdDvj6M9/XgAXAHMMXMrgJWAxcd8rcSEZEWpxMJRUTakGieSHgoR2GJiEgbpgAREZGIKEBERCQiChAREYmIAkRERCKiABERkYgoQEREJCIKEBERiYgCREREIqIAERGRiChAREQkIgoQERGJiAJEREQiogAREZGIKEBERCQiChAREYmIAkRERCKiABERkYgoQEREJCIKEBERiYgCREREIqIAERGRiChAREQkIgoQERGJiAJEREQiogAREZGIKEBERCQiChAREYlIowFiZhPNrNzMFjWo/crM1pnZR8Hj7Abv/dzMSs3sczM7o0H9zKBWamY/a1DvbWZzgvrzZpYazS8oIiItoylrIE8AZ+6nfq+7FwWP1wDM7BhgPHBsMM+DZhYysxDwAHAWcAxwSTAtwO+CZfUFtgJXHcoXEhGR1tFogLj7DGBLE5c3Dpjs7lXuvhIoBYYFj1J3X+Hu1cBkYJyZGTAWeCGY/0ng3GZ+BxERiYFD2Qdyk5l9HGzi6hzU8oG1DaYpC2oHqucAle5eu09dRETiXKQB8hBwJFAErAfujlpHB2Fm15pZiZmVVFRUtMZHiojIAUQUIO6+0d3r3L0eeJTwJiqAdUBBg0l7BrUD1TcDncwseZ/6gT53grsXu3txXl5eJK2LiEiURBQgZta9wcvzgL1HaE0FxptZmpn1BvoBHwJzgX7BEVephHe0T3V3B94BLgjmvwJ4JZKeRESkdSU3NoGZPQeMBnLNrAz4JTDazIoAB1YB1wG4+2IzmwJ8CtQCN7p7XbCcm4A3gRAw0d0XBx/xU2Cymf0GWAA8HrVvJyIiLcbCKwGJp7i42EtKSmLdhohIQjGzee5eHI1l6Ux0ERGJiAJEREQiogAREZGIKEBERCQiChAREYmIAkRERCKiABERkYgoQEREJCIKEBERiYgCREREIqIAERGRiChAREQkIgoQERGJiAJEREQiogAREZGIKEBERCQiChAREYmIAkRERCKiABERkYgoQEREJCIKEBERiYgCREREIqIAERGRiChAREQkIgoQERGJiAJEREQiogAREZGIKEBERCQiChAREYlIowFiZhPNrNzMFu3nvdvMzM0sN3g92sy2mdlHweN/N5j2TDP73MxKzexnDeq9zWxOUH/ezFKj9eVERKTlNGUN5AngzH2LZlYAfBtYs89bM929KHj8Opg2BDwAnAUcA1xiZscE0/8OuNfd+wJbgasi+SIiItK6Gg0Qd58BbNnPW/cC/wF4Ez5nGFDq7ivcvRqYDIwzMwPGAi8E0z0JnNuUxkVEJLaSI5nJzMYB69x9YTgDvmGkmS0EvgBud/fFQD6wtsE0ZcBwIAeodPfaBvX8g3zutcC1wcuq/W1Wi7IsYFsLz9vYdAd7/0Dv7Vvf33T71nKBTQft9NBFOp7NmS/a49mUWiKNZXPnjXQ8m1NvK+PZGv/W91fb9/WAg7fZDO7e6AMoBBYFz9sBc4Cs4IzbxKwAAAP6SURBVPUqIDd43hFoHzw/G1gWPL8AeKzB8i4D/kj4F6O0Qb1g7+c0oaeSpkx3KA9gQkvP29h0B3v/QO/tW9/fdPuZJm7HsznzRXs8mzh2CTOWrTWezam3lfFsjX/rrT2ekRyFdSTQG1hoZquAnsB8M+vm7tvdfQeAu78GpAQ72NcRDoe9ega1zUAnM0vepx4v/toK8zY23cHeP9B7+9b3N92hfLdIRfqZzZkv2uPZlFoijWVz5410PJtTbyvj2Rr/1vdXa7HxtCCRDj6RWSHwN3cfuJ/3VgHF7r7JzLoBG93dzWwY4X0bvYAQsBQ4lXBAzAUudffFZvZn4EV3n2xmDwMfu/uDTeipxN2Lm/g9pREaz+jRWEaXxjO6ojmeTTmM9zlgNjDAzMrM7GBHSV0ALAr2gdwHjPewWuAm4E3gM2CKh/eNAPwUuNXMSgnvE3m8ib1PaOJ00jQaz+jRWEaXxjO6ojaeTVoDERER2ZfORBcRkYgoQEREJCIKEBERichhFyBmlmRm/21m95vZFbHuJ9EF1zebaWYPm9noWPdzODCzTDMrMbNzYt1LojOzo4PfzRfM7IZY95PIzOxcM3s0uCbht5syT1wFyIEu3HigCzEewDjC55PUED6zvc2K0ng6sANIR+MZjfGE8JGHU1qmy8QRjfF098/c/XrgIuDEluw3nkVpLF9292uA64GLm/S58XQUlpmNIvzH6qm955wEF2JcCpxO+A/YXOASwueW/HafRfwgeGx190fM7AV3v6C1+o83URrPTe5eb2ZdgXvc/V9aq/94E6XxHEz4cPV0wmP7t9bpPv5EYzzdvdzMvgfcADzt7pNaq/94Eq2xDOa7G3jW3ec39rkRXQurpbj7jOCkxYa+uhAjgJlNBsa5+2+Bf9oEYGZlQHXwsq7luo1/0RjPBrYCaS3RZ6KI0u/naCCT8FWpd5vZa+5e35J9x6to/X66+1Rgqpm9CrTJAInS76YBdwCvNyU8IM4C5AAOdCHGA3kJuN/MTgZmtGRjCapZ42lm5wNnAJ0IX79MvqlZ4+nuvwAwsysJ1u5atLvE09zfz9HA+YT/c/Nai3aWeJr7t/Nm4DQgy8z6uvvDjX1AIgRIs7j7LnRPkahx95cIh7JEkbs/EeseDgfuPh2YHuM2Dgvufh/hK4g0WVztRD+AA12IUSKj8YwujWd0aTyjp8XHMhECZC7QL7j1bSowHpga454SmcYzujSe0aXxjJ4WH8u4CpD9XbixkQsxykFoPKNL4xldGs/oidVYxtVhvCIikjjiag1EREQShwJEREQiogAREZGIKEBERCQiChAREYmIAkRERCKiABERkYgoQEREJCIKEBERicj/BxGKTd6+faaMAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Loh9fstKcjT-"
      },
      "source": [
        "Training the model using the chosen learning rate value "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ja8NQJWpInIp"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load and prepare image dataset for training\n",
        "augmented_train_dataset, train_dataset, val_dataset, num_examples = get_datasets(test_size=TEST_SIZE)\n",
        "print(f\"Num of original examples: {num_examples}\")\n",
        "\n",
        "# Create a callback that saves the model's weights every few epochs during training\n",
        "checkpoint_path = './cp-cp{epoch:04d}.ckpt'\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_path,\n",
        "    verbose = 1,\n",
        "    save_weights_only = True,\n",
        "    save_freq = math.ceil(num_examples*(1-TEST_SIZE)/BATCH_SIZE) * 100\n",
        ")\n",
        "\n",
        "# Create custom callback to display outputs (via helper function) at the end of each epoch of training\n",
        "class CustomCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        # Generate random vector as test input to the decoder\n",
        "        random_vector_for_generation = tf.random.normal(shape=[8, LATENT_DIM])\n",
        "        # Generate and save images\n",
        "        print('End of epoch {} - mean loss = {}'.format(epoch, logs[keys[0]]))\n",
        "        display.clear_output(wait=False)\n",
        "        if epoch % 100 == 0:\n",
        "            generate_and_save_images(decoder, epoch, \n",
        "                                     math.ceil(num_examples/BATCH_SIZE), \n",
        "                                     random_vector_for_generation, vae)\n",
        "\n",
        "# Get the encoder, decoder and 'master' model (called vae)\n",
        "encoder, decoder, var_autoencoder = get_models(\n",
        "    input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS,), \n",
        "    latent_dim=LATENT_DIM)\n",
        "\n",
        "# Instantiate VAE class\n",
        "vae = VAE(encoder, decoder, var_autoencoder)\n",
        "\n",
        "# Compile model\n",
        "vae.compile(\n",
        "    optimizer = tf.keras.optimizers.Adam(lr=8e-4),\n",
        "    loss = tf.keras.losses.BinaryCrossentropy()\n",
        ")\n",
        "\n",
        "# Generate random vector as test input to the decoder\n",
        "random_vector_for_generation = tf.random.normal(shape=[8, LATENT_DIM])\n",
        "\n",
        "# Initialize the helper function to display outputs from an untrained model\n",
        "generate_and_save_images(decoder, 0, 0, random_vector_for_generation, vae)\n",
        "\n",
        "# Training loop using original dataset\n",
        "history = vae.fit(x=augmented_train_dataset, \n",
        "                  epochs=EPOCHS,  \n",
        "                  verbose=1, \n",
        "                  validation_data = val_dataset, \n",
        "                  callbacks=[cp_callback, CustomCallback()])      "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "vMTLorbeoCNu",
        "outputId": "30d562d7-3cab-40bf-b99e-fb4ff059a79b"
      },
      "source": [
        "plt.scatter(range(len(vae.history.history['loss'])), vae.history.history['loss'])\n",
        "plt.scatter(range(len(vae.history.history['val_loss'])), vae.history.history['val_loss'], color='red')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7fbf4e6fb5d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6sQRON4qRYB"
      },
      "source": [
        "# Show reconstructed images\n",
        "show_original_reconstructed_images(vae, train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffh7_Sr2cqp6"
      },
      "source": [
        "Instantiate new model, load weights from last checkpoint and resume training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBUrnwpMuCTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aabb3a8-8453-48ff-8930-33a377069a1c"
      },
      "source": [
        "# Get the encoder, decoder and 'master' model (called vae)\n",
        "encoder, decoder, var_autoencoder = get_models(input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS,), latent_dim=LATENT_DIM)\n",
        "\n",
        "# Create new instance of VAE class\n",
        "new_vae = VAE(encoder, decoder, var_autoencoder)\n",
        "\n",
        "# Load weights from last checkpoint\n",
        "checkpoint_dir = '.'\n",
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "print(latest)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./cp-cp0800.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7liWl_S9Wzzh"
      },
      "source": [
        "new_vae.load_weights(latest)\n",
        "new_vae.compile(\n",
        "    optimizer = tf.keras.optimizers.Adam(8e-4),\n",
        "    loss = tf.keras.losses.BinaryCrossentropy()\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbi_f_RFvE3T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e3cd95a-f130-451b-c49b-10b3769ee697"
      },
      "source": [
        "new_vae.evaluate(augmented_train_dataset, verbose=1)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25/25 [==============================] - 3s 89ms/step - loss: 13830.4582\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13828.001953125"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veT2g3x7u_0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f1dba420-e53a-46bc-a98a-30531019bca1"
      },
      "source": [
        "# Create a callback that saves the model's weights every few epochs during training\n",
        "checkpoint_path = './cp-cp{epoch:04d}.ckpt'\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_path,\n",
        "    verbose = 1,\n",
        "    save_weights_only = True,\n",
        "    save_freq = math.ceil(num_examples*(1-TEST_SIZE)/BATCH_SIZE) * 100\n",
        ")\n",
        "# Create custom callback to display outputs (via helper function) at the end of each epoch of training\n",
        "class CustomCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        # Generate random vector as test input to the decoder\n",
        "        random_vector_for_generation = tf.random.normal(shape=[8, LATENT_DIM])\n",
        "        # Generate and save images\n",
        "        display.clear_output(wait=True)\n",
        "        if epoch % 100 == 0:\n",
        "            generate_and_save_images(decoder, epoch, \n",
        "                                     math.ceil(num_examples/BATCH_SIZE), \n",
        "                                     random_vector_for_generation, new_vae)\n",
        "        print('End of epoch {} - mean loss = {}'.format(epoch, logs[keys[0]]))\n",
        "\n",
        "# Resume training using original dataset\n",
        "new_vae.fit(x=augmented_train_dataset, epochs=500, \n",
        "            verbose=1, \n",
        "            validation_data=val_dataset, \n",
        "            callbacks=[cp_callback, CustomCallback()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "End of epoch 226 - mean loss = 13159.765625\n",
            "Epoch 228/500\n",
            "25/25 [==============================] - ETA: 0s - loss: 13159.1133"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1tikmPh6yYh"
      },
      "source": [
        "plt.scatter(range(len(new_vae.history.history['loss'])), new_vae.history.history['loss'])\n",
        "plt.scatter(range(len(new_vae.history.history['loss'])), new_vae.history.history['val_loss'], color='red')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDT7Jm4ndBfB"
      },
      "source": [
        "Compare the original images with the reconstructed images from the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVLen6k1uIJ-"
      },
      "source": [
        "# Show reconstructed images\n",
        "show_original_reconstructed_images(vae, train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE_MPS_TPUStrategy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lamd91/VAE_DA/blob/master/VAE_MPS_TPUStrategy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlmkMoV9r0Zu"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "from IPython import display\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz65QZL4XPah"
      },
      "source": [
        "# Set up TPUs and initialize TPU strategy\n",
        "try:\n",
        "    tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu_address)\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "    print('Number of accelerators: ', strategy.num_replicas_in_sync)\n",
        "except ValueError:\n",
        "    print(\"TPU failed to initialize.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nejBn076s2So"
      },
      "source": [
        "# Define global constants\n",
        "BATCH_SIZE = 32\n",
        "LATENT_DIM = 2\n",
        "EPOCHS = 1000\n",
        "IMAGE_HEIGHT = 50\n",
        "IMAGE_WIDTH = 500\n",
        "NUM_CHANNELS = 1\n",
        "\n",
        "# Define decision variables for adding Cropping2D layers in decoder layers\n",
        "topcrop_after_upsampling1 = (math.ceil(math.ceil(IMAGE_HEIGHT/2)/2) % 2 != 0)\n",
        "leftcrop_after_upsampling1 = (math.ceil(math.ceil(IMAGE_WIDTH/2)/2) % 2 != 0)\n",
        "topcrop_after_upsampling2 = (math.ceil(IMAGE_HEIGHT/2) % 2 != 0)\n",
        "leftcrop_after_upsampling2 = (math.ceil(IMAGE_WIDTH/2) % 2 != 0)\n",
        "topcrop_after_upsampling3 = (IMAGE_HEIGHT % 2 != 0)\n",
        "leftcrop_after_upsampling3 = (IMAGE_WIDTH % 2 != 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klRQAqMks5PD"
      },
      "source": [
        "def map_image(image):\n",
        "    '''returns a reshaped tensor from a given image'''\n",
        "    image = tf.cast(image, dtype=tf.float32)\n",
        "    image = tf.reshape(image, \n",
        "    shape=(IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS,))\n",
        "\n",
        "    return image, image\n",
        "\n",
        "def get_datasets(map_fn, test_size):\n",
        "    \"\"\"Loads and prepares the dataset from a 2D array loaded from a text file.\"\"\"\n",
        "    dataset = np.transpose(np.loadtxt('/content/gdrive/My Drive/iniMPSimEns_1000.txt'))\n",
        "    num_examples = dataset.shape[0]\n",
        "    original_train_dataset = tf.data.Dataset.from_tensor_slices(dataset[0:int(num_examples*(1-test_size))])\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices(dataset[int(num_examples*(1-test_size)):])\n",
        "\n",
        "    original_train_dataset = original_train_dataset.map(map_fn)\n",
        "    val_dataset = val_dataset.map(map_fn)\n",
        "\n",
        "    train_dataset = original_train_dataset.batch(num_examples)\n",
        "    original_train_dataset = original_train_dataset.batch(BATCH_SIZE)\n",
        "    val_dataset = val_dataset.shuffle(1024).batch(BATCH_SIZE)\n",
        "\n",
        "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        #width_shift_range = 0.4,\n",
        "        horizontal_flip = True,\n",
        "        vertical_flip = True,\n",
        "        shear_range=0.2,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    for input_images, images in train_dataset:\n",
        "         x, y = input_images, images\n",
        "\n",
        "    train_generator = train_datagen.flow(x, y, batch_size=BATCH_SIZE)\n",
        "\n",
        "    return train_generator, original_train_dataset, val_dataset, num_examples\n",
        "\n",
        "def display_three_train_images(train_dataset):\n",
        "    \"\"\"Display 3 images from the training dataset\"\"\"\n",
        "    plt.figure(figsize=(5, 14))\n",
        "    for input_images, _ in train_dataset.take(1):\n",
        "        for i in range(3):\n",
        "            plt.subplot(3, 1, i+1)\n",
        "            plt.imshow(np.squeeze(input_images[i]), cmap='gray')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZReHg3QTtfe9"
      },
      "source": [
        "class Sampling(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Generates a random sample and combines with the encoder output\n",
        "\n",
        "        Args:\n",
        "          inputs -- output tensor from the encoder\n",
        "\n",
        "        Returns:\n",
        "          `inputs` tensors combined with a random sample\n",
        "        \"\"\"\n",
        "\n",
        "        # unpack the output of the encoder\n",
        "        mu, sigma = inputs\n",
        "\n",
        "        # get the size and dimensions of the batch\n",
        "        batch = tf.shape(mu)[0]\n",
        "        dim = tf.shape(mu)[1]\n",
        "\n",
        "        # generate a random tensor\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "\n",
        "        # combine the inputs and noise\n",
        "        return mu + tf.exp(0.5 * sigma) * epsilon\n",
        "\n",
        "def encoder_layers(inputs, latent_dim):\n",
        "    \"\"\"Defines the encoder's layers.\n",
        "    Args:\n",
        "      inputs -- batch from the dataset\n",
        "      latent_dim -- dimensionality of the latent space\n",
        "\n",
        "    Returns:\n",
        "      mu -- learned mean\n",
        "      sigma -- learned standard deviation\n",
        "      batch_2.shape -- shape of the features before flattening\n",
        "    \"\"\"\n",
        "\n",
        "    # add the Conv2D layers followed by BatchNormalization\n",
        "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding=\"same\", activation='relu',\n",
        "                               name=\"encode_conv1\")(inputs)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu',\n",
        "                               name=\"encode_conv2\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=2, padding='same', activation='relu',\n",
        "                               name=\"encode_conv3\")(x)\n",
        "\n",
        "    # assign to a different variable so you can extract the shape later\n",
        "    batch_3 = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    # flatten the features and feed into the Dense network\n",
        "    x = tf.keras.layers.Flatten(name=\"encode_flatten\")(batch_3)\n",
        "\n",
        "    # we arbitrarily used 256 units here but feel free to change\n",
        "    x = tf.keras.layers.Dense(1024, activation='relu', name=\"encode_dense\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    # add output Dense networks for mu and sigma, units equal to the declared latent_dim.\n",
        "    mu = tf.keras.layers.Dense(latent_dim, name='latent_mu')(x)\n",
        "    sigma = tf.keras.layers.Dense(latent_dim, name='latent_sigma')(x)\n",
        "\n",
        "    return mu, sigma, batch_3.shape\n",
        "\n",
        "def encoder_model(latent_dim, input_shape):\n",
        "    \"\"\"Defines the encoder model with the Sampling layer\n",
        "    Args:\n",
        "      latent_dim -- dimensionality of the latent space\n",
        "      input_shape -- shape of the dataset batch\n",
        "\n",
        "    Returns:\n",
        "      model -- the encoder model\n",
        "      conv_shape -- shape of the features before flattening\n",
        "    \"\"\"\n",
        "\n",
        "    # declare the inputs tensor with the given shape\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "    # get the output of the encoder_layers() function\n",
        "    mu, sigma, conv_shape = encoder_layers(inputs, latent_dim=LATENT_DIM)\n",
        "\n",
        "    # feed mu and sigma to the Sampling layer\n",
        "    z = Sampling()((mu, sigma))\n",
        "\n",
        "    # build the whole encoder model\n",
        "    model = tf.keras.Model(inputs, outputs=[mu, sigma, z])\n",
        "\n",
        "    return model, conv_shape\n",
        "\n",
        "def decoder_layers(inputs, conv_shape, topcrop_after_upsampling1, \n",
        "                   leftcrop_after_upsampling1, topcrop_after_upsampling2, \n",
        "                   leftcrop_after_upsampling2,\n",
        "                   topcrop_after_upsampling3, leftcrop_after_upsampling3):\n",
        "    \"\"\"Defines the decoder layers.\n",
        "    Args:\n",
        "      inputs -- output of the encoder\n",
        "      conv_shape -- shape of the features before flattening\n",
        "\n",
        "    Returns:\n",
        "      tensor containing the decoded output\n",
        "    \"\"\"\n",
        "\n",
        "    # feed to a Dense network with units computed from the conv_shape dimensions\n",
        "    units = conv_shape[1] * conv_shape[2] * conv_shape[3]\n",
        "    x = tf.keras.layers.Dense(units, activation='relu', name=\"decode_dense1\")(inputs)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    # reshape output using the conv_shape dimensions\n",
        "    x = tf.keras.layers.Reshape((conv_shape[1], conv_shape[2], conv_shape[3]), name=\"decode_reshape\")(x)\n",
        "\n",
        "    # upsample the features back to the original dimensions\n",
        "    # for that, make sure to add Cropping2D layers after upsampling when needed\n",
        "    x = tf.keras.layers.Conv2DTranspose(filters=128, kernel_size=3, strides=2, padding='same', activation='relu',\n",
        "                                        name=\"decode_conv2d_1\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    if topcrop_after_upsampling1:\n",
        "        x = tf.keras.layers.Cropping2D(cropping=((1, 0), (0, 0)))(x)\n",
        "    if leftcrop_after_upsampling1:\n",
        "        x = tf.keras.layers.Cropping2D(cropping=((0, 0), (1, 0)))(x)\n",
        "    x = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same', activation='relu',\n",
        "                                        name=\"decode_conv2d_2\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    if topcrop_after_upsampling2:\n",
        "        x = tf.keras.layers.Cropping2D(cropping=((1, 0), (0, 0)))(x)\n",
        "    if leftcrop_after_upsampling2:\n",
        "        x = tf.keras.layers.Cropping2D(cropping=((0, 0), (1, 0)))(x)\n",
        "    x = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=2, padding='same', activation='relu',\n",
        "                                        name=\"decode_conv2d_3\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    if topcrop_after_upsampling3:\n",
        "        x = tf.keras.layers.Cropping2D(cropping=((1, 0), (0, 0)))(x)\n",
        "    if leftcrop_after_upsampling3:\n",
        "        x = tf.keras.layers.Cropping2D(cropping=((0, 0), (1, 0)))(x)\n",
        "    x = tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=1, padding='same', activation='sigmoid',\n",
        "                                        name=\"decode_final\")(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def decoder_model(latent_dim, conv_shape):\n",
        "    \"\"\"Defines the decoder model.\n",
        "    Args:\n",
        "      latent_dim -- dimensionality of the latent space\n",
        "      conv_shape -- shape of the features before flattening\n",
        "\n",
        "    Returns:\n",
        "      model -- the decoder model\n",
        "    \"\"\"\n",
        "\n",
        "    # set the inputs to the shape of the latent space\n",
        "    inputs = tf.keras.layers.Input(shape=(latent_dim,))\n",
        "\n",
        "    # get the output of the decoder layers\n",
        "    outputs = decoder_layers(inputs, conv_shape, topcrop_after_upsampling1, \n",
        "                             leftcrop_after_upsampling1, \n",
        "                             topcrop_after_upsampling2, \n",
        "                             leftcrop_after_upsampling2, \n",
        "                             topcrop_after_upsampling3, \n",
        "                             leftcrop_after_upsampling3)\n",
        "\n",
        "    # declare the inputs and outputs of the model\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "def kl_reconstruction_loss(inputs, outputs, mu, sigma):\n",
        "    \"\"\" Computes the Kullback-Leibler Divergence (KLD)\n",
        "    Args:\n",
        "      inputs -- batch from the dataset\n",
        "      outputs -- output of the Sampling layer\n",
        "      mu -- mean\n",
        "      sigma -- standard deviation\n",
        "\n",
        "    Returns:\n",
        "      KLD loss\n",
        "    \"\"\"\n",
        "    kl_loss = 1 + sigma - tf.square(mu) - tf.math.exp(sigma)\n",
        "    kl_loss = tf.reduce_mean(kl_loss) * -0.5\n",
        "\n",
        "    return kl_loss\n",
        "\n",
        "def vae_model(encoder, decoder, input_shape):\n",
        "    \"\"\"Defines the VAE model\n",
        "    Args:\n",
        "      encoder -- the encoder model\n",
        "      decoder -- the decoder model\n",
        "      input_shape -- shape of the dataset batch\n",
        "\n",
        "    Returns:\n",
        "      the complete VAE model\n",
        "    \"\"\"\n",
        "    # set the inputs\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "    # get mu, sigma, and z from the encoder output\n",
        "    mu, sigma, z = encoder(inputs)\n",
        "\n",
        "    # get reconstructed output from the decoder\n",
        "    reconstructed = decoder(z)\n",
        "\n",
        "    # define the inputs and outputs of the VAE\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=reconstructed)\n",
        "\n",
        "    # add the KL loss\n",
        "    loss = kl_reconstruction_loss(inputs, z, mu, sigma)\n",
        "    model.add_loss(loss)\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_models(input_shape, latent_dim):\n",
        "    \"\"\"Returns the encoder, decoder, and vae models\"\"\"\n",
        "    encoder, conv_shape = encoder_model(latent_dim=latent_dim, input_shape=input_shape)\n",
        "    decoder = decoder_model(latent_dim=latent_dim, conv_shape=conv_shape)\n",
        "    vae = vae_model(encoder, decoder, input_shape=input_shape)\n",
        "    return encoder, decoder, vae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kuHjYojtmHz"
      },
      "source": [
        "# Define a VAE class via model subclassing\n",
        "loss_metrics = tf.keras.metrics.Mean()\n",
        "\n",
        "class VAE(tf.keras.Model):\n",
        "    def __init__(self, encoder, decoder, variational_autoencoder):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.vae = variational_autoencoder\n",
        "\n",
        "    # override train_step method\n",
        "    def train_step(self, images):\n",
        "        if isinstance(images, tuple):\n",
        "            images = images[0]\n",
        "        with tf.GradientTape() as tape:\n",
        "            # feed a batch to the VAE model\n",
        "            reconstructed = self.vae(images)\n",
        "            # compute reconstruction loss\n",
        "            flattened_inputs = tf.reshape(images, [-1])\n",
        "            flattened_outputs = tf.reshape(reconstructed, [-1])\n",
        "            loss = self.compiled_loss(flattened_inputs, flattened_outputs) \\\n",
        "                   * images.shape[1] * images.shape[2]\n",
        "            # add KLD regularization loss\n",
        "            loss += sum(self.vae.losses)\n",
        "\n",
        "        # compute the gradients and update the model weights\n",
        "        grads = tape.gradient(loss, self.vae.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.vae.trainable_weights))\n",
        "\n",
        "        # update metrics\n",
        "        loss_metrics.update_state(loss)\n",
        "        \n",
        "        # return a dict mapping metrics names to current value\n",
        "        return {'loss': loss_metrics.result()}\n",
        "\n",
        "    # override test_step method\n",
        "    def test_step(self, images):\n",
        "        if isinstance(images, tuple):\n",
        "            images = images[0]\n",
        "        # compute predictions\n",
        "        reconstructed = self.vae(images)\n",
        "        # compute loss\n",
        "        flattened_inputs = tf.reshape(images, [-1])\n",
        "        flattened_outputs = tf.reshape(reconstructed, [-1])\n",
        "        loss = self.compiled_loss(flattened_inputs, flattened_outputs) \\\n",
        "               * images.shape[1] * images.shape[2]\n",
        "        # add KLD regularization loss\n",
        "        loss += sum(self.vae.losses)\n",
        "        # update metrics\n",
        "        loss_metrics.update_state(loss)\n",
        "        # return a dict mapping metrics names to current value\n",
        "        return {'loss': loss_metrics.result()}\n",
        "\n",
        "    def call(self, images):\n",
        "        if isinstance(images, tuple):\n",
        "            images = images[0]\n",
        "        return self.vae(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqBdfidbt3aF"
      },
      "source": [
        "def generate_and_save_images(model, epoch, step, test_input):\n",
        "    \"\"\"Helper function to plot our 8 images\n",
        "\n",
        "    Args:\n",
        "\n",
        "    model -- the decoder model\n",
        "    epoch -- current epoch number during training\n",
        "    step -- current step number during training\n",
        "    test_input -- random tensor with shape (8, LATENT_DIM)\n",
        "    \"\"\"\n",
        "\n",
        "    # generate images from the test input\n",
        "    predictions = model.predict(test_input)\n",
        "\n",
        "    # plot the results\n",
        "    fig = plt.figure(figsize=(12, 14))\n",
        "\n",
        "    for i in range(predictions.shape[0]):\n",
        "        plt.subplot(8, 1, i + 1)\n",
        "        plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
        "        plt.axis('off')\n",
        "\n",
        "    # tight_layout minimizes the overlap between 2 sub-plots\n",
        "    fig.suptitle(\"epoch: {}, step: {}\".format(epoch, step))\n",
        "    plt.savefig('image_at_epoch_{:04d}_step{:04d}.png'.format(epoch, step))\n",
        "    plt.show()\n",
        "\n",
        "def show_original_reconstructed_images(model, train_dataset):\n",
        "    plt.figure(figsize=(10, 14))\n",
        "    for input_images, _ in train_dataset.take(1):\n",
        "        reconstructed = model(input_images)\n",
        "        k = 0\n",
        "        for i in range(5):\n",
        "            reconstructed_categorized = np.where(reconstructed >= 0.5, 1, 0)\n",
        "            plt.subplot(5, 2, k+1)\n",
        "            plt.imshow(np.squeeze(input_images[i]), cmap='gray')\n",
        "            plt.subplot(5, 2, k+2)\n",
        "            plt.imshow(np.squeeze(reconstructed_categorized[i]), cmap='gray')\n",
        "            #plt.imshow(np.squeeze(reconstructed[i]), cmap='gray')\n",
        "            k += 2\n",
        "        plt.savefig(\"reconstructed_images.png\")\n",
        "    plt.show()\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwJjP5p7t7Ob"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load and prepare image dataset for training\n",
        "train_generator, train_dataset, val_dataset, num_examples = get_datasets(map_image, test_size=0.2)\n",
        "print(f\"Num of original examples: {num_examples}\")\n",
        "#display_three_train_images(train_dataset)\n",
        "\n",
        "# Create a callback that saves the model's weights every few epochs during training\n",
        "checkpoint_path = 'checkpoint/cp-cp{epoch:04d}.ckpt'\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_path,\n",
        "    verbose = 1,\n",
        "    save_weights_only = True,\n",
        "    save_freq = math.ceil(num_examples/BATCH_SIZE) * 10\n",
        ")\n",
        "\n",
        "# Create custom callback to display outputs (via helper function) at the end of each epoch of training\n",
        "class CustomCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        # Generate random vector as test input to the decoder\n",
        "        random_vector_for_generation = tf.random.normal(shape=[8, LATENT_DIM])\n",
        "        # Generate and save images\n",
        "        display.clear_output(wait=False)\n",
        "        generate_and_save_images(decoder, epoch, math.ceil(num_examples/BATCH_SIZE), random_vector_for_generation)\n",
        "        print('End of epoch {} - mean loss = {}'.format(epoch, logs[keys[0]]))\n",
        "\n",
        "# Create callback for adjusting learning rate during training\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-5 * 10**(epoch / 30))\n",
        "\n",
        "# Get the encoder, decoder and 'master' model (called vae)\n",
        "encoder, decoder, var_autoencoder = get_models(input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS,), latent_dim=LATENT_DIM)\n",
        "\n",
        "# Instantiate VAE class\n",
        "vae = VAE(encoder, decoder, var_autoencoder)\n",
        "\n",
        "# Compile model\n",
        "vae.compile(\n",
        "    optimizer = tf.keras.optimizers.Adam(lr=1e-5),\n",
        "    loss = tf.keras.losses.BinaryCrossentropy()\n",
        ")\n",
        "\n",
        "# Generate random vector as test input to the decoder\n",
        "random_vector_for_generation = tf.random.normal(shape=[8, LATENT_DIM])\n",
        "\n",
        "# Initialize the helper function to display outputs from an untrained model\n",
        "#generate_and_save_images(decoder, 0, 0, random_vector_for_generation)\n",
        "\n",
        "# Training loop using original dataset\n",
        "history = vae.fit(train_dataset, epochs=100, batch_size=BATCH_SIZE, verbose=1, callbacks=[cp_callback, CustomCallback(), lr_schedule])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BSZVxR26H6D"
      },
      "source": [
        "# Plot losses against learning rates\n",
        "plt.semilogx(history.history['lr'], history.history['loss'])\n",
        "plt.axis([1e-5, 0.005, 14500, 17400])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ja8NQJWpInIp"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load and prepare image dataset for training\n",
        "train_generator, train_dataset, val_dataset, num_examples = get_datasets(map_image, test_size=0.2)\n",
        "print(f\"Num of original examples: {num_examples}\")\n",
        "\n",
        "# Create a callback that saves the model's weights every few epochs during training\n",
        "checkpoint_path = './cp-cp{epoch:04d}.ckpt'\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_path,\n",
        "    verbose = 1,\n",
        "    save_weights_only = True,\n",
        "    save_freq = math.ceil(num_examples/BATCH_SIZE) * 30\n",
        ")\n",
        "\n",
        "# Create custom callback to display outputs (via helper function) at the end of each epoch of training\n",
        "class CustomCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        # Generate random vector as test input to the decoder\n",
        "        random_vector_for_generation = tf.random.normal(shape=[8, LATENT_DIM])\n",
        "        # Generate and save images\n",
        "        print('End of epoch {} - mean loss = {}'.format(epoch, logs[keys[0]]))\n",
        "        display.clear_output(wait=True)\n",
        "        generate_and_save_images(decoder, epoch, math.ceil(num_examples/BATCH_SIZE), random_vector_for_generation)\n",
        "\n",
        "# Get the encoder, decoder and 'master' model (called vae)\n",
        "encoder, decoder, var_autoencoder = get_models(input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS,), latent_dim=LATENT_DIM)\n",
        "\n",
        "# Instantiate VAE class\n",
        "vae = VAE(encoder, decoder, var_autoencoder)\n",
        "\n",
        "# Compile model\n",
        "vae.compile(\n",
        "    optimizer = tf.keras.optimizers.Adam(lr=1e-4),\n",
        "    loss = tf.keras.losses.BinaryCrossentropy()\n",
        ")\n",
        "\n",
        "# Generate random vector as test input to the decoder\n",
        "random_vector_for_generation = tf.random.normal(shape=[8, LATENT_DIM])\n",
        "\n",
        "# Initialize the helper function to display outputs from an untrained model\n",
        "#generate_and_save_images(decoder, 0, 0, random_vector_for_generation)\n",
        "\n",
        "# Training loop using original dataset\n",
        "history = vae.fit(x=train_dataset, epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
        "                  verbose=1, validation_data = val_dataset, \n",
        "                  callbacks=[cp_callback, CustomCallback()])      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMTLorbeoCNu"
      },
      "source": [
        "plt.scatter(range(len(vae.history.history['loss'])), vae.history.history['loss'])\n",
        "plt.scatter(range(len(vae.history.history['loss'])), vae.history.history['val_loss'], color='red')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBUrnwpMuCTv"
      },
      "source": [
        "# Get the encoder, decoder and 'master' model (called vae)\n",
        "encoder, decoder, var_autoencoder = get_models(input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS,), latent_dim=LATENT_DIM)\n",
        "\n",
        "# Create new instance of VAE class\n",
        "new_vae = VAE(encoder, decoder, var_autoencoder)\n",
        "\n",
        "# Load weights from last checkpoint\n",
        "checkpoint_dir = 'checkpoint'\n",
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "print(latest)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7liWl_S9Wzzh"
      },
      "source": [
        "new_vae.load_weights(latest)\n",
        "new_vae.compile(\n",
        "    optimizer = tf.keras.optimizers.Adam(3e-4),\n",
        "    loss = tf.keras.losses.BinaryCrossentropy()\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbi_f_RFvE3T"
      },
      "source": [
        "new_vae.evaluate(train_dataset, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veT2g3x7u_0b"
      },
      "source": [
        "# Create a callback that saves the model's weights every few epochs during training\n",
        "checkpoint_path = './cp-cp{epoch:04d}.ckpt'\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_path,\n",
        "    verbose = 1,\n",
        "    save_weights_only = True,\n",
        "    save_freq = math.ceil(num_examples/BATCH_SIZE) * 30\n",
        ")\n",
        "# Create custom callback to display outputs (via helper function) at the end of each epoch of training\n",
        "class CustomCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        # Generate random vector as test input to the decoder\n",
        "        random_vector_for_generation = tf.random.normal(shape=[8, LATENT_DIM])\n",
        "        # Generate and save images\n",
        "        display.clear_output(wait=True)\n",
        "        generate_and_save_images(decoder, epoch, math.ceil(num_examples/BATCH_SIZE), random_vector_for_generation)\n",
        "        print('End of epoch {} - mean loss = {}'.format(epoch, logs[keys[0]]))\n",
        "\n",
        "# Resume training using original dataset\n",
        "new_vae.fit(x=train_dataset, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1, validation_data=val_dataset, callbacks=[cp_callback, CustomCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXOSEpR5u9Nj"
      },
      "source": [
        "# Resume training using augmented dataset\n",
        "new_vae.fit(train_generator, epochs=1000, verbose=1, callbacks=[cp_callback, CustomCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVLen6k1uIJ-"
      },
      "source": [
        "# Show reconstructed images\n",
        "show_original_reconstructed_images(vae, train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}